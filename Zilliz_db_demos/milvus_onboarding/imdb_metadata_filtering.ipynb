{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab684ec",
   "metadata": {},
   "source": [
    "## Read CSV data into a pandas dataframe\n",
    "\n",
    "The data used in this notebook is [Kaggle 48K movies](https://www.kaggle.com/datasets/yashgupta24/48000-movies-dataset) which contains a lot of metadata in addition to the raw review text.\n",
    "\n",
    "Usually there is a data cleaning step.  Such as replace empty strings with \"\" or unusual and empty fields with median values.  Below, I'll just drop rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b26d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab install these libraries in this order:\n",
    "# !pip install numpy pandas torch pymilvus langchain transformers sentence-transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12cc6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries.\n",
    "import sys, os, time, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import custom functions for splitting and search.\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "import milvus_utilities as _utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecab0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48513\n",
      "45036\n",
      "Example text length: 6556\n",
      "('Example text: Sallie Gardner at a Gallop Sallie Gardner at a Gallop is a '\n",
      " 'short starring Gilbert Domm and Sallie Gardner. The clip shows a jockey, '\n",
      " 'Domm, riding a horse, Sally Gardner. The clip is not filmed but instead '\n",
      " 'consists of 24 individual photographs shot in rapid... Sometimes ascribed as '\n",
      " '\"The Father of the Motion Picture\", Eadweard Muybridge undeniably '\n",
      " 'accomplished exploiting and sometimes introducing a means of instantaneous '\n",
      " 'and serial images to analyze and synthesize animal locomotion. In part, the '\n",
      " \"reasons for and the claims made of his work support Virgilio Tosi's thesis \"\n",
      " 'that cinema was invented out of the needs of scientific research. '\n",
      " \"Conversely, they're informed by Muybridge's background as an artistic \"\n",
      " 'location photographer and, as Phillip Prodger suggests, in book sales and '\n",
      " 'more useful to art than to science, as Marta Braun has demonstrated (see '\n",
      " 'sources at bottom). Additionally, Muybridge quickly exploited their '\n",
      " 'entertainment value via projection to audiences across the U.S. and Europe. '\n",
      " 'Muybridge pursued both of these paths of invention: the path taken by Jules '\n",
      " 'Janssen, Étienne-Jules Marey and others for science and the path taken by '\n",
      " 'Ottomar Anschütz, Thomas Edison, the Lumiére brothers and others for fame '\n",
      " 'and profit.\\n'\n",
      " '\\n'\n",
      " 'Muybridge began taking instantaneous single photographs of multi-millionaire '\n",
      " \"railroad magnate Leland Stanford's horses in motion in 1872. It was disputed \"\n",
      " \"at the time whether all four of a horse's legs were off the ground \"\n",
      " 'simultaneously at any time while running. Although no surviving photographs '\n",
      " 'prove it, contemporary lithographs and paintings likely based on the '\n",
      " 'photographs, indeed, show the moment of \"unsupported transit\". In between '\n",
      " 'and interrupting these experiments, Muybridge was found not guilty of the '\n",
      " \"admittedly premeditated fatal shooting of his wife's lover and possibly her \"\n",
      " \"son's father.\\n\"\n",
      " '\\n'\n",
      " \"Publication of Marey's graphic measurements of a horse's movements reignited \"\n",
      " \"Stanford's interest in the gait of horses. In turn, Marey was convinced to \"\n",
      " \"switch to photography in his motion studies after witnessing Muybridge's \"\n",
      " 'work (see \"Falling Cat\" (1894)). This work in \"automatic '\n",
      " 'electro-photographs\" began in 1878 at Stanford\\'s Palo Alto Stock Farm. '\n",
      " 'Multiple cameras were stored in a shed parallel to a track. A series of '\n",
      " 'closing boards serving as shutters were triggered by tripped threads and '\n",
      " 'electrical means. The wet collodion process of the time, reportedly, could '\n",
      " 'need up to half a minute for an exposure. For the split-second shutter '\n",
      " 'speeds required here, a white canvas background and powdered lime on the '\n",
      " 'track provided more contrast to compensate for less light getting to the '\n",
      " \"glass plates. Employees of Stanford's Central Pacific Railroad and others \"\n",
      " 'helped in constructing this \"set\" and camera equipment.\\n'\n",
      " '\\n'\n",
      " 'Contrary to unattributed claims on the web, this so-called \"Sallie Gardner '\n",
      " 'at a Gallop\" wasn\\'t the first series photographed by Muybridge. Six series '\n",
      " 'of Muybridge\\'s first subjects were published on cards entitled \"The Horse '\n",
      " 'in Motion\". The first is of the horse Abe Edgington trotting on 11 June '\n",
      " '1878. Reporters were invited for the next two series on June 15th, and, as '\n",
      " 'they reported, again, Abe went first—trotting and pulling the driver behind '\n",
      " 'in a sulky, which is what tripped the threads. The second subject that day '\n",
      " 'was Sallie Gardner running and, thus, the mare had to trip the threads. '\n",
      " 'Reporters noted how this spooked her and how that was reflected in the '\n",
      " 'negatives developed on the spot. As one article said, she \"gave a wild bound '\n",
      " 'in the air, breaking the saddle girth as she left the ground.\" Based on such '\n",
      " \"descriptions, it doesn't seem that this series exists anymore. The \"\n",
      " 'animations on the web that are actually of Sallie are dated June 19th on '\n",
      " '\"The Horse in Motion\" card. Many animations claimed to be Sallie on YouTube, '\n",
      " 'Wikipedia and elsewhere, as of this date, are actually of a mare named Annie '\n",
      " \"G. and were part of Muybridge's University of Pennsylvania work published in \"\n",
      " '1887, as the Library of Congress and other reliable sources have made clear. '\n",
      " \"The early Palo Alto photographs aren't as detailed and are closer to \"\n",
      " \"silhouettes. The 12 images of Gardner also include one where she's \"\n",
      " \"stationary. The Morse's Gallery pictures are entirely in silhouette, while \"\n",
      " 'the La Nature engravings of these same images show the rider in a white '\n",
      " 'shirt.\\n'\n",
      " '\\n'\n",
      " 'The shot of the horse stationary, as Braun points out, was added later and '\n",
      " 'is indicative of the artistic and un-scientific assemblages Muybridge made '\n",
      " 'of his images—with the intent of publication, including in his own books. '\n",
      " 'This was especially prominent in his Pennsylvania work, which included many '\n",
      " 'nude models that were surely useful for art. Muybridge influenced artists '\n",
      " 'from Realists like Thomas Eakins and Meissonier, Impressionists like Edgar '\n",
      " 'Degas and Frederick Remington, to the more abstract works of Francis Bacon. '\n",
      " 'His precedence has also been cited in the photography of Steven Pippin and '\n",
      " 'Hollis Frampton, as well as the bullet-time effects in \"The Matrix\" (1999).\\n'\n",
      " '\\n'\n",
      " 'Muybridge lectured on this relationship with art when touring with his '\n",
      " 'Zoöpraxiscope, which was a combination of the magic lantern and '\n",
      " 'phenakistoscope. With it, he projected, from glass disks, facsimiles of his '\n",
      " 'photographs hand-painted by Erwin Faber. Without intermittent movement, the '\n",
      " 'Zoöpraxiscope compressed the images, so elongated drawings were used instead '\n",
      " 'of photographs. Muybridge and others also used his images for '\n",
      " 'phenakistoscopes and zoetropes. The first demonstration of the Zoöpraxiscope '\n",
      " 'was to Stanford and friends in the autumn of 1879. A public demonstration '\n",
      " 'was given on 4 May 1880 for the San Francisco art association, and Muybridge '\n",
      " 'continued these lectures for years—personally touring the U.S. and Europe. '\n",
      " 'Although there were predecessors in animated projections as far back as 1847 '\n",
      " 'by Leopold Ludwig Döbler, in 1853 by Franz von Uchatius, and with posed '\n",
      " 'photographs by Henry Heyl in 1870, the chronophotographic and artistic basis '\n",
      " \"offered some novelty for Muybridge's presentations. They also led him to \"\n",
      " 'meet Edison and Marey and inspire the likes of Anschütz and others—those who '\n",
      " 'took the next steps in the invention of movies.\\n'\n",
      " '\\n'\n",
      " '(Main Sources: \"The Inventor and the Tycoon\" by Edward Ball. \"Eadweard '\n",
      " 'Muybridge\" and \"Picturing Time\" by Marta Braun. \"The Man Who Stopped Time\" '\n",
      " 'by Brian Clegg. \"Man in Motion\" by Robert Bartlett Haas. \"The Father of the '\n",
      " 'Motion Picture\" by Gordon Hendricks. \"The Stanford Years, 1872-1882\" edited '\n",
      " 'by Anita Ventura Mozley. \"Time Stands Still\" by Phillip Prodger. \"Cinema '\n",
      " 'Before Cinema\" by Virgilio Tosi.)')\n",
      "id               int64\n",
      "url             object\n",
      "Name            object\n",
      "PosterLink      object\n",
      "Genres          object\n",
      "Actors          object\n",
      "Director        object\n",
      "Keywords        object\n",
      "RatingValue    float32\n",
      "text            object\n",
      "MovieYear        int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>Name</th>\n",
       "      <th>PosterLink</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Actors</th>\n",
       "      <th>Director</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>RatingValue</th>\n",
       "      <th>text</th>\n",
       "      <th>MovieYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47836</th>\n",
       "      <td>47836</td>\n",
       "      <td>https://www.imdb.com/title/tt5842890/</td>\n",
       "      <td>Jupiter holdja</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BNjY0Ym...</td>\n",
       "      <td>[Drama, Fantasy, Mystery, Sci-Fi, Thriller]</td>\n",
       "      <td>[Merab Ninidze, Zsombor Jéger, György Cserhalm...</td>\n",
       "      <td>Kornél Mundruczó</td>\n",
       "      <td>[rear nudity, bare chested male, levitation, d...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Jupiter holdja Jupiter holdja is a movie starr...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47837</th>\n",
       "      <td>47837</td>\n",
       "      <td>https://www.imdb.com/title/tt5843670/</td>\n",
       "      <td>The Stakelander</td>\n",
       "      <td>https://m.media-amazon.com/images/M/MV5BMTY2Nz...</td>\n",
       "      <td>[Action, Drama, Horror, Sci-Fi]</td>\n",
       "      <td>[Connor Paolo, Nick Damici, Laura Abramsen, A....</td>\n",
       "      <td>Dan Berk,Robert Olsen</td>\n",
       "      <td>[crying man, mother daughter relationship, cam...</td>\n",
       "      <td>5.3</td>\n",
       "      <td>The Stakelander The Stakelander is a movie sta...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                    url             Name  \\\n",
       "47836  47836  https://www.imdb.com/title/tt5842890/   Jupiter holdja   \n",
       "47837  47837  https://www.imdb.com/title/tt5843670/  The Stakelander   \n",
       "\n",
       "                                              PosterLink  \\\n",
       "47836  https://m.media-amazon.com/images/M/MV5BNjY0Ym...   \n",
       "47837  https://m.media-amazon.com/images/M/MV5BMTY2Nz...   \n",
       "\n",
       "                                            Genres  \\\n",
       "47836  [Drama, Fantasy, Mystery, Sci-Fi, Thriller]   \n",
       "47837              [Action, Drama, Horror, Sci-Fi]   \n",
       "\n",
       "                                                  Actors  \\\n",
       "47836  [Merab Ninidze, Zsombor Jéger, György Cserhalm...   \n",
       "47837  [Connor Paolo, Nick Damici, Laura Abramsen, A....   \n",
       "\n",
       "                    Director  \\\n",
       "47836       Kornél Mundruczó   \n",
       "47837  Dan Berk,Robert Olsen   \n",
       "\n",
       "                                                Keywords  RatingValue  \\\n",
       "47836  [rear nudity, bare chested male, levitation, d...          6.0   \n",
       "47837  [crying man, mother daughter relationship, cam...          5.3   \n",
       "\n",
       "                                                    text  MovieYear  \n",
       "47836  Jupiter holdja Jupiter holdja is a movie starr...       2017  \n",
       "47837  The Stakelander The Stakelander is a movie sta...       2017  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import common libraries.\n",
    "import sys, os, time, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import custom functions for splitting and search.\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path.\n",
    "import milvus_utilities as _utils\n",
    "\n",
    "# Read CSV data.\n",
    "df = pd.read_csv(\"data/original_data.csv\")\n",
    "\n",
    "# Concatenate 'Name', 'Keywords', and 'Description' into 'text' column\n",
    "df['text'] = df['Name'] + ' ' + df['Description'] + ' ' + df['ReviewBody']\n",
    "# drop rows without any text.\n",
    "print(df.shape[0])\n",
    "df = df.dropna(subset=['text'])\n",
    "print(df.shape[0])\n",
    "\n",
    "# Convert genres from string with commas in it to list of strings.\n",
    "df['Genres'] = df['Genres'].str.split(',')\n",
    "df['Genres'] = df['Genres'].apply(lambda d: d if isinstance(d, list) else [\"\"])\n",
    "\n",
    "# Convert actors from string with commas in it to list of strings.\n",
    "df['Actors'] = df['Actors'].str.split(',')\n",
    "df['Actors'] = df['Actors'].apply(lambda d: d if isinstance(d, list) else [\"\"])\n",
    "\n",
    "# Convert keywords from string with commas in it to list of strings.\n",
    "df['Keywords'] = df['Keywords'].str.split(',')\n",
    "df['Keywords'] = df['Keywords'].apply(lambda d: d if isinstance(d, list) else [\"\"])\n",
    "\n",
    "# Extract out just the year from the date.\n",
    "def extract_year(movie_date):\n",
    "    try:\n",
    "        return int(movie_date.split('-')[0])\n",
    "    except Exception:\n",
    "        return -1  # return -1 instead of None\n",
    "df['MovieYear'] = df.DatePublished.apply(extract_year)\n",
    "\n",
    "# Convert 'Rating' to float.\n",
    "df['RatingValue'] = pd.to_numeric(df['RatingValue'], errors='coerce')\n",
    "df['RatingValue'] = df['RatingValue'].fillna(-1).astype('float32')\n",
    "\n",
    "# Drop extra rating columns.\n",
    "df.drop(columns=['RatingCount', 'BestRating', 'WorstRating',\n",
    "                 'ReviewAurthor', 'ReviewDate',\t'ReviewBody',\t\n",
    "                 'Description', 'duration', 'DatePublished'], inplace=True)\n",
    "\n",
    "# Inspect text.\n",
    "print(f\"Example text length: {len(df.text[0])}\")\n",
    "pprint.pprint(f\"Example text: {df.text[0]}\")\n",
    "\n",
    "# Shortcut the data for testing.\n",
    "df = df.tail(600).copy()\n",
    "\n",
    "print(df.dtypes)\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19e525",
   "metadata": {},
   "source": [
    "## Connect using Milvus Lite\n",
    "\n",
    "Milvus Lite is a light Python server that can run locally.  It's ideal for getting started with Milvus, running on a laptop, in a Jupyter notebook, or on Colab. \n",
    "\n",
    "⛔️ Please note Milvus Lite is only meant for demos, not for production workloads.\n",
    "\n",
    "- [github](https://github.com/milvus-io/milvus-lite)\n",
    "- [documentation](https://milvus.io/docs/quickstart.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80de564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymilvus:2.4.3\n"
     ]
    }
   ],
   "source": [
    "# STEP 1. CONNECT TO MILVUS LITE.\n",
    "\n",
    "# !python -m pip install -U --no-cache-dir pymilvus\n",
    "import pymilvus\n",
    "print(f\"pymilvus:{pymilvus.__version__}\")\n",
    "\n",
    "# Connect a client to the Milvus Lite server.\n",
    "from pymilvus import MilvusClient\n",
    "mc = MilvusClient(\"milvus_demo.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963eacb3",
   "metadata": {},
   "source": [
    "# Optional - Connect using Milvus standalone Docker\n",
    "\n",
    "This section uses [Milvus standalone](https://milvus.io/docs/configure-docker.md) on Docker. <br>\n",
    ">⛔️ Make sure you pip install the correct version of pymilvus and server yml file.  **Versions (major and minor) should all match**.\n",
    "\n",
    "1. [Install Docker](https://docs.docker.com/get-docker/)\n",
    "2. Start your Docker Desktop\n",
    "3. Download the latest [docker-compose.yml](https://milvus.io/docs/install_standalone-docker.md#Download-the-YAML-file) (or run the wget command, replacing version to what you are using)\n",
    "> wget https://github.com/milvus-io/milvus/releases/download/v2.4.0-rc.1/milvus-standalone-docker-compose.yml -O docker-compose.yml\n",
    "4. From your terminal:  \n",
    "   - cd into directory where you saved the .yml file (usualy same dir as this notebook)\n",
    "   - docker compose up -d\n",
    "   - verify (either in terminal or on Docker Desktop) the containers are running\n",
    "5. From your code (see notebook code below):\n",
    "   - Import milvus\n",
    "   - Connect to the local milvus server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba692141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # STEP 1. CONNECT TO MILVUS STANDALONE DOCKER.\n",
    "# import pymilvus\n",
    "\n",
    "# # print(f\"Pymilvus: {pymilvus.__version__}\") #2.4.3\n",
    "# # !wget https://github.com/milvus-io/milvus/releases/download/v2.4.1/milvus-standalone-docker-compose.yml -O docker-compose.yml\n",
    "\n",
    "# import pymilvus, time\n",
    "# from pymilvus import (MilvusClient, utility, connections)\n",
    "# print(f\"Pymilvus: {pymilvus.__version__}\")\n",
    "\n",
    "# # Start the Milvus server.\n",
    "# # !docker compose up -d\n",
    "\n",
    "# # Connect to the local server.\n",
    "# connection = connections.connect(\n",
    "#   alias=\"default\", \n",
    "#   host='localhost', # or '0.0.0.0' or 'localhost'\n",
    "#   port='19530'\n",
    "# )\n",
    "\n",
    "# # Get server version.\n",
    "# print(f\"Milvus server: {utility.get_server_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d6622",
   "metadata": {},
   "source": [
    "## Load the Embedding Model checkpoint and use it to create vector embeddings\n",
    "**Embedding model:**  We will use the open-source [sentence transformers](https://www.sbert.net/docs/pretrained_models.html) available on HuggingFace to encode the documentation text.  We will download the model from HuggingFace and run it locally. \n",
    "\n",
    "💡Tip:  A good way to choose a sentence transformer model is to check the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).  Sort descending by column \"Retrieval Average\" and choose the best-performing small model.\n",
    "\n",
    "Two model parameters of note below:\n",
    "1. EMBEDDING_DIM refers to the dimensionality or length of the embedding vector. In this case, the embeddings generated for EACH token in the input text will have the SAME length = 1024. This size of embedding is often associated with BERT-based models, where the embeddings are used for downstream tasks such as classification, question answering, or text generation. <br><br>\n",
    "2. MAX_SEQ_LENGTH is the maximum Context Length the encoder model can handle for input sequences. In this case, if sequences longer than 512 tokens are given to the model, everything longer will be (silently!) chopped off.  This is the reason why a chunking strategy is needed to segment input texts into chunks with lengths that will fit in the model's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe46dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -U sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2be7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py311-unum/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: BAAI/bge-large-en-v1.5\n",
      "EMBEDDING_DIM: 1024\n",
      "MAX_SEQ_LENGTH: 512\n"
     ]
    }
   ],
   "source": [
    "# STEP 2. DOWNLOAD AN OPEN SOURCE EMBEDDING MODEL.\n",
    "\n",
    "# Import torch.\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize torch settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model from huggingface model hub.\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "\n",
    "# Get the model parameters and save for later.\n",
    "EMBEDDING_DIM = encoder.get_sentence_embedding_dimension()\n",
    "MAX_SEQ_LENGTH_IN_TOKENS = encoder.get_max_seq_length() \n",
    "# Assume tokens are 3 characters long.\n",
    "MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS * 3\n",
    "EOS_TOKEN_LENGTH = 1 * 3\n",
    "\n",
    "# Use 512 sequence length.\n",
    "MAX_SEQ_LENGTH = MAX_SEQ_LENGTH_IN_TOKENS\n",
    "EOS_TOKEN_LENGTH = 1\n",
    "\n",
    "# Inspect model parameters.\n",
    "print(f\"model_name: {model_name}\")\n",
    "print(f\"EMBEDDING_DIM: {EMBEDDING_DIM}\")\n",
    "print(f\"MAX_SEQ_LENGTH: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Milvus collection\n",
    "\n",
    "You can think of a collection in Milvus like a \"table\" in SQL databases.  The **collection** will contain the \n",
    "- **Schema** (or [no-schema Milvus client](https://milvus.io/docs/using_milvusclient.md)).  \n",
    "💡 You'll need the vector `EMBEDDING_DIM` parameter from your embedding model.\n",
    "Typical values are:\n",
    "   - 1024 for sbert embedding models\n",
    "   - 1536 for ada-002 OpenAI embedding models\n",
    "- **Vector index** for efficient vector search\n",
    "- **Vector distance metric** for measuring nearest neighbor vectors\n",
    "- **Consistency level**\n",
    "In Milvus, transactional consistency is possible; however, according to the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem), some latency must be sacrificed. 💡 Searching movie reviews is not mission-critical, so [`eventually`](https://milvus.io/docs/consistency.md) consistent is fine here.\n",
    "\n",
    "## Add a Vector Index\n",
    "\n",
    "The vector index determines the vector **search algorithm** used to find the closest vectors in your data to the query a user submits.  \n",
    "\n",
    "Most vector indexes use different sets of parameters depending on whether the database is:\n",
    "- **inserting vectors** (creation mode) - vs - \n",
    "- **searching vectors** (search mode) \n",
    "\n",
    "Scroll down the [docs page](https://milvus.io/docs/index.md) to see a table listing different vector indexes available on Milvus.  For example:\n",
    "- FLAT - deterministic exhaustive search\n",
    "- IVF_FLAT or IVF_SQ8 - Hash index (stochastic approximate search)\n",
    "- HNSW - Graph index (stochastic approximate search)\n",
    "- AUTOINDEX - Automatically determined based on OSS vs [Zilliz cloud](https://docs.zilliz.com/docs/autoindex-explained), type of GPU, size of data.\n",
    "\n",
    "Besides a search algorithm, we also need to specify a **distance metric**, that is, a definition of what is considered \"close\" in vector space.  In the cell below, the [`HNSW`](https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md) search index is chosen.  Its possible distance metrics are one of:\n",
    "- L2 - L2-norm\n",
    "- IP - Dot-product\n",
    "- COSINE - Angular distance\n",
    "\n",
    "💡 Most use cases work better with normalized embeddings, in which case L2 is useless (every vector has length=1) and IP and COSINE are the same.  Only choose L2 if you plan to keep your embeddings unnormalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dropped collection: `IMDB_metadata`\n",
      "Successfully created collection: `IMDB_metadata`\n"
     ]
    }
   ],
   "source": [
    "# STEP 3. CREATE A NO-SCHEMA MILVUS COLLECTION AND DEFINE THE DATABASE INDEX.\n",
    "\n",
    "# Create a collection.\n",
    "COLLECTION_NAME = \"IMDB_metadata\"\n",
    "\n",
    "# Milvus Lite uses the MilvusClient object.\n",
    "if mc.has_collection(COLLECTION_NAME):\n",
    "    mc.drop_collection(COLLECTION_NAME)\n",
    "    print(f\"Successfully dropped collection: `{COLLECTION_NAME}`\")\n",
    "\n",
    "# # Check if collection already exists, if so drop it.\n",
    "# has = utility.has_collection(COLLECTION_NAME)\n",
    "# if has:\n",
    "#     drop_result = utility.drop_collection(COLLECTION_NAME)\n",
    "#     print(f\"Successfully dropped collection: `{COLLECTION_NAME}`\")\n",
    "# # Use no-schema Milvus client uses flexible json key:value format.\n",
    "# mc = MilvusClient(connections=connection)\n",
    "\n",
    "# Create a collection with flexible schema and AUTOINDEX.\n",
    "mc.create_collection(COLLECTION_NAME, \n",
    "                     EMBEDDING_DIM,\n",
    "                     consistency_level=\"Eventually\", \n",
    "                     auto_id=True,  \n",
    "                     overwrite=True,\n",
    "                    )\n",
    "print(f\"Successfully created collection: `{COLLECTION_NAME}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60423a5",
   "metadata": {},
   "source": [
    "## Simple Chunking\n",
    "\n",
    "Before embedding, it is necessary to decide your chunk strategy, chunk size, and chunk overlap.  This section uses:\n",
    "- **Strategy** = Simple fixed chunk lengths.\n",
    "- **Chunk size** = Use the embedding model's parameter `MAX_SEQ_LENGTH`\n",
    "- **Overlap** = Rule-of-thumb 10-15%\n",
    "- **Function** = \n",
    "  - Langchain's `RecursiveCharacterTextSplitter` to split up long reviews recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9179a45",
   "metadata": {},
   "source": [
    "## Insert data into Milvus\n",
    "\n",
    "For each original text chunk, we'll write the quadruplet (`vector, text, source, h1, h2`) into the database.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/db_insert.png\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "**The Milvus Client wrapper can only handle loading data from a list of dictionaries.**\n",
    "\n",
    "Otherwise, in general, Milvus supports loading data from:\n",
    "- pandas dataframes \n",
    "- list of dictionaries\n",
    "\n",
    "Below, we use the embedding model provided by HuggingFace, download its checkpoint, and run it locally as the encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ee61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for chunking and embedding.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def recursive_splitter_wrapper(text, chunk_size):\n",
    "\n",
    "    # Default chunk overlap is 10% chunk_size.\n",
    "    chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "\n",
    "    # Use langchain's convenient recursive chunking method.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\", \"\\n\\n\", \"<br /><br />\"],\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Use recursive splitter to chunk text.\n",
    "def imdb_chunk_text(df_batch, chunk_size):\n",
    "\n",
    "    batch = df_batch.copy()\n",
    "    print(f\"original shape: {batch.shape}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Chunk the text review into chunk_size.\n",
    "    batch['chunk'] = batch['text'].apply(recursive_splitter_wrapper, chunk_size=chunk_size)\n",
    "    # Explode the 'chunk' column to create new rows for each chunk.\n",
    "    batch = batch.explode('chunk', ignore_index=True)\n",
    "    print(f\"new shape: {batch.shape}\")\n",
    "\n",
    "    # 2. Add embeddings as new column in df.\n",
    "    embeddings = torch.tensor(encoder.encode(batch['chunk']))\n",
    "    # Normalize the embeddings.\n",
    "    embeddings = np.array(embeddings / np.linalg.norm(embeddings))\n",
    "\n",
    "    # 3. Convert embeddings to list of `numpy.ndarray`, each containing `numpy.float32` numbers.\n",
    "    converted_values = list(map(np.float32, embeddings))\n",
    "    batch['vector'] = converted_values\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Chunking + embedding time for {batch.shape[0]} docs: {end_time - start_time} sec\")\n",
    "    # Inspect the batch of data.\n",
    "    # assert len(batch.chunk[0]) <= MAX_SEQ_LENGTH-1\n",
    "    assert len(batch.vector[0]) == EMBEDDING_DIM\n",
    "    # print(f\"type embeddings: {type(batch.vector)} of {type(batch.vector[0])}\")\n",
    "    # print(f\"of numbers: {type(batch.vector[0][0])}\")\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9cf495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk size: 512\n",
      "original shape: (200, 11)\n",
      "new shape: (443, 12)\n",
      "Chunking + embedding time for 443 docs: 36.468486070632935 sec\n",
      "Start inserting entities\n",
      "Milvus Client insert time for 443 vectors: 0.1802201271057129 seconds\n",
      "\n",
      "original shape: (200, 11)\n",
      "new shape: (394, 12)\n",
      "Chunking + embedding time for 394 docs: 33.942102909088135 sec\n",
      "Start inserting entities\n",
      "Milvus Client insert time for 394 vectors: 0.11721491813659668 seconds\n",
      "\n",
      "original shape: (200, 11)\n",
      "new shape: (433, 12)\n",
      "Chunking + embedding time for 433 docs: 35.6629958152771 sec\n",
      "Start inserting entities\n",
      "Milvus Client insert time for 433 vectors: 0.14162302017211914 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Chunk and Embed Text Data\n",
    "\n",
    "# Use the embedding model parameters.\n",
    "# chunk_size = MAX_SEQ_LENGTH - HF_EOS_TOKEN_LENGTH\n",
    "chunk_size = 512\n",
    "chunk_overlap = np.round(chunk_size * 0.10, 0)\n",
    "BATCH_SIZE = 200\n",
    "print(f\"chunk size: {chunk_size}\")\n",
    "\n",
    "for i in range(0, len(df), BATCH_SIZE):\n",
    "\n",
    "    # STEP 5. CHUNK AND EMBED TEXT DATA.\n",
    "    batch = imdb_chunk_text(df.iloc[i:i+BATCH_SIZE], chunk_size)\n",
    "    # display(batch.head(2))\n",
    "\n",
    "    # Drop the original text column, keep the new 'chunk' column.\n",
    "    batch.drop(columns=['id','text'], axis=1, inplace=True)\n",
    "\n",
    "    # STEP 6. INSERT CHUNK LIST INTO MILVUS OR ZILLIZ.\n",
    "    # Convert the DataFrame to a list of dictionaries\n",
    "    dict_list = batch.to_dict(orient='records')\n",
    "\n",
    "    # Insert data into the Milvus collection.\n",
    "    print(\"Start inserting entities\")\n",
    "    start_time = time.time()\n",
    "    insert_result = mc.insert(\n",
    "        COLLECTION_NAME,\n",
    "        data=dict_list,\n",
    "        progress_bar=True)\n",
    "    end_time = time.time()\n",
    "    print(f\"Milvus Client insert time for {batch.shape[0]} vectors: {end_time - start_time} seconds\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da855b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [\"{'count(*)': 1270}\"] , extra_info: {'cost': 0}\n",
      "timing: 0.0021 seconds\n"
     ]
    }
   ],
   "source": [
    "# Count rows.\n",
    "start_time = time.time()\n",
    "res = mc.query( collection_name=COLLECTION_NAME, \n",
    "               filter=\"\", \n",
    "               output_fields = [\"count(*)\"], )\n",
    "pprint.pprint(res)\n",
    "end_time = time.time()\n",
    "print(f\"timing: {round(end_time - start_time, 4)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb04eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n",
      "<class 'dict'> 11\n",
      "{'Actors': ['Gay Talese', 'Gerald Foos', 'Nan Talese', 'Susan Morrison'],\n",
      " 'Director': 'Myles Kane,Josh Koury',\n",
      " 'Genres': ['Documentary'],\n",
      " 'Keywords': ['male frontal nudity', 'male nudity', 'voyeur'],\n",
      " 'MovieYear': 2017,\n",
      " 'Name': 'Voyeur',\n",
      " 'PosterLink': 'https://m.media-amazon.com/images/M/MV5BODE2OTg1NDQ0M15BMl5BanBnXkFtZTgwMjg0NTAxNDM@._V1_.jpg',\n",
      " 'RatingValue': 6.199999809265137,\n",
      " 'chunk': 'Voyeur Voyeur is a movie starring Gay Talese, Gerald Foos, and Nan '\n",
      "          'Talese. Journalism icon Gay Talese reports on Gerald Foos, the '\n",
      "          'owner of a Colorado motel, who allegedly secretly watched his '\n",
      "          'guests with the aid of specially designed... \"Voyeur,\" a '\n",
      "          'documentary purportedly about an infamous voyeur, should really be '\n",
      "          'titled \"Pseudo-Journalist,\" since what it really demonstrates is '\n",
      "          'what a terrible writer Gay Talese is. It\\'s not \"true crime,\" since '\n",
      "          'so many of the claims in it are debatable or demonstrably false.',\n",
      " 'url': 'https://www.imdb.com/title/tt7588790/',\n",
      " 'vector': array([ 1.3714395e-03, -2.1723497e-03, -7.7940949e-04, ...,\n",
      "        7.9407757e-05, -1.3696894e-03,  7.3388114e-04], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# TODO - Uncomment to inspect the first chunk and its metadata.\n",
    "print(len(dict_list))\n",
    "print(type(dict_list[0]), len(dict_list[0]))\n",
    "pprint.pprint(dict_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c022c38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'url',\n",
       " 'Name',\n",
       " 'PosterLink',\n",
       " 'Genres',\n",
       " 'Actors',\n",
       " 'Director',\n",
       " 'Keywords',\n",
       " 'RatingValue',\n",
       " 'MovieYear',\n",
       " 'chunk']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define metadata fields you can filter on.\n",
    "OUTPUT_FIELDS = list(dict_list[0].keys())\n",
    "# drop vector field\n",
    "OUTPUT_FIELDS.remove('vector')\n",
    "OUTPUT_FIELDS.insert(0, 'id')\n",
    "OUTPUT_FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "129bc5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Comedy',\n",
       " 'Sci-Fi',\n",
       " 'History',\n",
       " 'Music',\n",
       " 'Drama',\n",
       " 'Horror',\n",
       " 'War',\n",
       " 'Musical',\n",
       " 'Documentary',\n",
       " 'Family',\n",
       " 'Biography',\n",
       " 'Romance',\n",
       " 'Thriller',\n",
       " 'Short',\n",
       " 'Fantasy',\n",
       " 'News',\n",
       " 'Mystery',\n",
       " 'Adventure',\n",
       " 'Animation',\n",
       " 'Action',\n",
       " 'Western',\n",
       " 'Crime',\n",
       " 'Sport']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List distinct genres\n",
    "GENRES = list(set([genre for genres in df['Genres'] for genre in genres]))\n",
    "GENRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e84931bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of rating values.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4, 2))\n",
    "df['RatingValue'].hist();\n",
    "# Scale of 1-10.  Popular movies are rated >= 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c589ff",
   "metadata": {},
   "source": [
    "## Ask a question about your data\n",
    "\n",
    "So far in this demo notebook: \n",
    "1. Your custom data has been mapped into a vector embedding space\n",
    "2. Those vector embeddings have been saved into a vector database\n",
    "\n",
    "Next, you can ask a question about your custom data!\n",
    "\n",
    "💡 In LLM vocabulary:\n",
    "> **Query** is the generic term for user questions.  \n",
    "A query is a list of multiple individual questions, up to maybe 1000 different questions!\n",
    "\n",
    "> **Question** usually refers to a single user question.  \n",
    "In our example below, the user question is \"What is AUTOINDEX in Milvus Client?\"\n",
    "\n",
    "> **Semantic Search** = very fast search of the entire knowledge base to find the `TOP_K` documentation chunks with the closest embeddings to the user's query.\n",
    "\n",
    "💡 The same model should always be used for consistency for all the embeddings data and the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e7f41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample question about your data.\n",
    "\n",
    "# This question for the JSON dataset.\n",
    "SAMPLE_QUESTION = \"Dystopia science fiction with a robot.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea29411",
   "metadata": {},
   "source": [
    "## Execute a vector search\n",
    "\n",
    "Search Milvus using [PyMilvus API](https://milvus.io/docs/search.md).\n",
    "\n",
    "💡 By their nature, vector searches are \"semantic\" searches.  For example, if you were to search for \"leaky faucet\": \n",
    "> **Traditional Key-word Search** - either or both words \"leaky\", \"faucet\" would have to match some text in order to return a web page or link text to the document.\n",
    "\n",
    "> **Semantic search** - results containing words \"drippy\" \"taps\" would be returned as well because these words mean the same thing even though they are different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9673ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a convenience function for searching.\n",
    "def mc_run_search(question, filter_expression, top_k):\n",
    "    # Embed the question using the same encoder.\n",
    "    query_embeddings = _utils.embed_query(encoder, [question])\n",
    "\n",
    "    # Run semantic vector search using your query and the vector database.\n",
    "    results = mc.search(\n",
    "        COLLECTION_NAME,\n",
    "        data=query_embeddings, \n",
    "        # search_params=SEARCH_PARAMS,\n",
    "        output_fields=OUTPUT_FIELDS, \n",
    "        # Milvus can utilize metadata in boolean expressions to filter search.\n",
    "        filter=filter_expression,\n",
    "        # expr=filter_expression,\n",
    "        limit=top_k,\n",
    "        consistency_level=\"Eventually\"\n",
    "    )\n",
    "\n",
    "    # Assemble retrieved context and context metadata.\n",
    "    # The search result is in the variable `results[0]`, which is type \n",
    "    # 'pymilvus.orm.search.SearchResult'. \n",
    "    METADATA_FIELDS = [f for f in OUTPUT_FIELDS if f != 'chunk']\n",
    "    formatted_results, context, context_metadata = _utils.client_assemble_retrieved_context(\n",
    "        results, metadata_fields=METADATA_FIELDS, num_shot_answers=top_k)\n",
    "    \n",
    "    return formatted_results, context, context_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e25ccac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter: RatingValue >= 7 && (Name like \"%Panther%\")\n",
      "Milvus Client search time for 433 vectors: 0.20736312866210938 seconds\n",
      "type: <class 'list'>, count: 2\n"
     ]
    }
   ],
   "source": [
    "# STEP 7. RETRIEVE ANSWERS FROM YOUR DOCUMENTS STORED IN MILVUS OR ZILLIZ.\n",
    "\n",
    "TOP_K = 10\n",
    "# Metadata filters for CSV dataset.\n",
    "expression = \"\"\n",
    "expression='RatingValue >= 7'\n",
    "# Infix string match.\n",
    "expression=expression + ' && (Name like \"%Panther%\")'\n",
    "# Exact array element with prefix or infix match.\n",
    "# expression=expression + ' && (Genres[0] like \"Sci-Fi%\" || Genres[1] like \"%Sci-Fi%\" || Genres[2] == \"Sci-Fi\")'\n",
    "# String array contains a value, not working.\n",
    "# https://milvus.io/docs/array_data_type.md#Advanced-filtering\n",
    "# expression=expression + ' && ARRAY_CONTAINS_ANY(Genres, [\"%Sci-Fi%\"])'\n",
    "print(f\"filter: {expression}\")\n",
    "\n",
    "start_time = time.time()\n",
    "formatted_results, contexts, context_metadata = \\\n",
    "    mc_run_search(SAMPLE_QUESTION, expression, TOP_K)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Milvus Client search time for {len(dict_list)} vectors: {elapsed_time} seconds\")\n",
    "\n",
    "# Inspect search result.\n",
    "print(f\"type: {type(formatted_results)}, count: {len(formatted_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb53d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved result #1\n",
      "distance = 0.45058321952819824\n",
      "movie_index: 450537193043396268\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://m.media-amazon.com/images/M/MV5BMTg1MTY2MjYzNV5BMl5BanBnXkFtZTgwMTc4NTMwNDI@._V1_.jpg\" width=\"150\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chunk: It has no special story, and the plot is totally predictable. Colors '\n",
      " 'and visuals are the only gorgeous parts of this film. I felt cheated after '\n",
      " 'how amazingly Black Panther was shown in Civil War.\\n'\n",
      " '\\n'\n",
      " 'It has been released in 3D but there are merely a few shots that felt good '\n",
      " 'in 3D.\\n'\n",
      " '\\n'\n",
      " 'Overall acting is okay; no actor leaves their mark on your mind. And as '\n",
      " 'always, this Marvel movie too suffers with the villain deficiency syndrome.\\n'\n",
      " '\\n'\n",
      " \"6 out of 10 from me, that's it.\")\n",
      "id: 450537193043396268\n",
      "url: https://www.imdb.com/title/tt1825683/\n",
      "Name: Black Panther\n",
      "Genres: ['Action', 'Adventure', 'Sci-Fi']\n",
      "Actors: ['Chadwick Boseman', 'Michael B. Jordan', \"Lupita Nyong'o\", 'Danai Gurira']\n",
      "Director: Ryan Coogler\n",
      "Keywords: ['marvel cinematic universe', 'marvel comics', 'based on comic book', 'king', 'african king']\n",
      "RatingValue: 7.300000190734863\n",
      "MovieYear: 2018\n",
      "\n",
      "Retrieved result #2\n",
      "distance = 0.3965144753456116\n",
      "movie_index: 450537193043396267\n"
     ]
    }
   ],
   "source": [
    "# Display poster link.\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Loop through recommended movies, display poster, print metadata.\n",
    "seen_movies = []\n",
    "for i in range(len(contexts)):\n",
    "    print(f\"Retrieved result #{i+1}\")\n",
    "    print(f\"distance = {formatted_results[i][0]}\")\n",
    "    # Get the movie_index\n",
    "    movie_index = context_metadata[i]['id']\n",
    "    movie_name = context_metadata[i]['Name']\n",
    "    print(f\"movie_index: {movie_index}\")\n",
    "\n",
    "    # Don't display the same movie_index twice.\n",
    "    if movie_name in seen_movies:\n",
    "        continue\n",
    "    else:\n",
    "        seen_movies.append(movie_name)\n",
    "        # Display the first poster link as a rendered image\n",
    "        x = Image(url = context_metadata[i]['PosterLink'], width=150, height=200) \n",
    "        display(x)\n",
    "\n",
    "        # Print the rest of the movie info.\n",
    "        pprint.pprint(f\"chunk: {contexts[i]}\")\n",
    "        # print metadata except the movie_index and poster link.\n",
    "        for key, value in context_metadata[i].items():\n",
    "            if ((key != 'PosterLink') and (key != 'movie_index')):\n",
    "                print(f\"{key}: {value}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6294947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the collection.\n",
    "# mc.drop_collection(COLLECTION_NAME)\n",
    "# print(f\"Successfully dropped collection: `{COLLECTION_NAME}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c777937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Christy Bergman\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.11.8\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "torch                : 2.3.0\n",
      "transformers         : 4.41.2\n",
      "sentence_transformers: 2.6.1\n",
      "pymilvus             : 2.4.3\n",
      "langchain            : 0.2.2\n",
      "\n",
      "conda environment: py311-unum\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Props to Sebastian Raschka for this handy watermark.\n",
    "# !pip install watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a 'Christy Bergman' -v -p torch,transformers,sentence_transformers,pymilvus,langchain --conda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
