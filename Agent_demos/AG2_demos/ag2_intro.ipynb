{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro ag2, formerly Autogen\n",
    "- [website](https://ag2.ai/)\n",
    "- [github](https://github.com/ag2ai/ag2)\n",
    "- [docs](https://docs.ag2.ai/docs/home/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade ag2\n",
    "# !python -m pip install --upgrade pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autogen: 0.2.25\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os, autogen\n",
    "from autogen import ConversableAgent #controller agent\n",
    "\n",
    "print(f\"autogen: {autogen.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM to be the control agent\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm_config = {\"model\": \"gpt-4o-mini\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Q&A, single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n",
      "\n",
      "Sure, I can help with that! However, I need to know which joke you would like me to repeat. Could you please provide it?\n"
     ]
    }
   ],
   "source": [
    "# Generate a chat from an agent, without instantiation.\n",
    "# This does not use the ConversableAgent class.\n",
    "# There is no internal state!  So no memory of previous messages.\n",
    "\n",
    "# Instantiate the ConversableAgent\n",
    "agent = ConversableAgent(\n",
    "    name=\"chatbot\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "# Generate a single session message.\n",
    "reply = agent.generate_reply(\n",
    "    messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}]\n",
    ")\n",
    "print(reply)\n",
    "\n",
    "# Generate another single session message.\n",
    "reply = agent.generate_reply(\n",
    "    messages=[{\"content\": \"Repeat the joke.\", \"role\": \"user\"}]\n",
    ")\n",
    "print()\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple, Multi-agents with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cathy = ConversableAgent(\n",
    "    name=\"cathy\",\n",
    "    system_message=\n",
    "    \"Your name is Cathy and you are a stand-up comedian. \"\n",
    "    \"When you're ready to end the conversation, say 'I gotta go'.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    # \"stop words\" to end conversation, other agent won't reply.\n",
    "    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"],\n",
    ")\n",
    "\n",
    "joe = ConversableAgent(\n",
    "    name=\"joe\",\n",
    "    system_message=\n",
    "    \"Your name is Joe and you are a stand-up comedian. \"\n",
    "    \"When you're ready to end the conversation, say 'I gotta go'.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    # \"stop words\" to end conversation, other agent won't reply.\n",
    "    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"] or \"Goodbye\" in msg[\"content\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "I'm Joe. Cathy, let's keep the jokes rolling.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "Hey Joe! Alright, let‚Äôs keep that laughter flowing! What did the zero say to the eight? \"Nice belt!\" What‚Äôs your favorite type of joke?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "Hey! I love that joke‚Äîclassic! My favorite type of joke has to be puns. They‚Äôre like the dad bod of humor: a little cheesy but always brings a smile. Got any more jokes up your sleeve?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "Absolutely, Joe! I‚Äôm all about that pun life! Here‚Äôs one for you: I used to be a baker, but I couldn't make enough dough. Now I'm just kneading some laughs instead! Got any favorites of your own?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "That‚Äôs a good one! You really kneaded that laugh, huh? One of my favorites is: I told my wife she was drawing her eyebrows too high. She looked surprised! It‚Äôs all in the delivery! Got any more puns to share?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "Haha, that‚Äôs a great one! You‚Äôre right, delivery is key! Alright, how about this: I told my computer I needed a break, and now it won‚Äôt stop sending me beach wallpapers! If only life had an ‚Äúundo‚Äù button, right? Keep 'em coming if you‚Äôve got more!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjoe\u001b[0m (to cathy):\n",
      "\n",
      "That‚Äôs hilarious! I love how tech can be so helpful‚Äîand so intrusive! Here‚Äôs one for you: Why did the scarecrow win an award? Because he was outstanding in his field! Life really needs more punchlines, don‚Äôt you think? Got another for me?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "Oh, that‚Äôs a classic! Scarecrows really know how to work that field! Here‚Äôs another: I used to play piano by ear, but now I use my hands. Much easier, don‚Äôt you think? Life could definitely use a soundtrack of punchlines! What else ya got for me?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate w/Joe asking Cathy for a joke, and run 2 turns.\n",
    "# Uses a controller agent, so memory of interactions is retained.\n",
    "# Summarize information so far and pass forward to next turn.\n",
    "\n",
    "# The agent flow interactions are defined by 1 initial message.\n",
    "chat_result = joe.initiate_chat(\n",
    "    recipient=cathy, \n",
    "    message=\"I'm Joe. Cathy, let's keep the jokes rolling.\",\n",
    "    max_turns=4, \n",
    "    # If you want to be able to inspect/summarize this.\n",
    "    summary_method=\"reflection_with_llm\",\n",
    "    summary_prompt=\"Summarize the conversation\",\n",
    ")\n",
    "\n",
    "# Automatically uses caching! So same input = same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mcathy\u001b[0m (to joe):\n",
      "\n",
      "What's last joke we talked about?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# memory?\n",
    "cathy.send(message=\"What's last joke we talked about?\", \n",
    "recipient=joe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel, Sequential Agents\n",
    "\n",
    "Pattern requires an agent acting like PM, to carryover conversations.\n",
    "\n",
    "E.g. 4 agents, including 1 manager.\n",
    "- A = Designer\n",
    "- B = Planner\n",
    "- C = Formatter\n",
    "- D = Manager\n",
    "\n",
    "Pattern is:\n",
    "A <> B, C <> B, D<>B, ->Summarizer, ->Chat result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM to be the control agent\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm_config = {\"model\": \"gpt-4o-mini\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 4 worker bots for a newsfeed, including manager.\n",
    "\n",
    "# Get name, location, works in background\n",
    "onboarding_personal_information_agent = ConversableAgent(\n",
    "    name=\"Onboarding Personal Information Agent\",\n",
    "    system_message='''You are a helpful customer onboarding agent,\n",
    "    you are here to help new customers get started with our product.\n",
    "    Your job is to gather customer's name and location.\n",
    "    Do not ask for other information. Return 'TERMINATE' \n",
    "    when you have gathered all the information.''',\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Get news preferences, works in background\n",
    "onboarding_topic_preference_agent = ConversableAgent(\n",
    "    name=\"Onboarding Topic preference Agent\",\n",
    "    system_message='''You are a helpful customer onboarding agent,\n",
    "    you are here to help new customers get started with our product.\n",
    "    Your job is to gather customer's preferences on news topics.\n",
    "    Do not ask for other information.\n",
    "    Return 'TERMINATE' when you have gathered all the information.''',\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Newsfeed generation agent, works in background\n",
    "customer_engagement_agent = ConversableAgent(\n",
    "    name=\"Customer Engagement Agent\",\n",
    "    system_message='''You are a helpful customer service agent\n",
    "    here to provide fun for the customer based on the user's\n",
    "    personal information and topic preferences.\n",
    "    This could include fun facts, jokes, or interesting stories.\n",
    "    Make sure to make it engaging and fun!\n",
    "    Return 'TERMINATE' when you are done.''',\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
    ")\n",
    "\n",
    "# Manager is also front facing, always asks for human input.\n",
    "customer_proxy_agent = ConversableAgent(\n",
    "    system_message='''You talk to customers. ''',\n",
    "    name=\"customer_proxy_agent\",\n",
    "    llm_config=False,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent-to-agent flow using order of chats.\n",
    "chats = [\n",
    "    {\n",
    "        \"sender\": onboarding_personal_information_agent,\n",
    "        \"recipient\": customer_proxy_agent,\n",
    "        \"message\": \n",
    "            '''Hello, I'm here to help you get started with our product.\n",
    "            Could you tell me your name and location?''',\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "        \"summary_args\": {\n",
    "            \"summary_prompt\" : \"Return the customer information \"\n",
    "                             \"into as JSON object only: \"\n",
    "                             \"{'name': '', 'location': ''}\",\n",
    "        },\n",
    "        \"max_turns\": 2,\n",
    "        \"clear_history\" : True\n",
    "    },\n",
    "    {\n",
    "        \"sender\": onboarding_topic_preference_agent,\n",
    "        \"recipient\": customer_proxy_agent,\n",
    "        \"message\": \n",
    "                \"Great! Could you tell me what topics you are \"\n",
    "                \"interested in reading about?\",\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "        \"summary_args\": {\n",
    "            \"summary_prompt\" : \"Return the customer information \"\n",
    "                             \"into as JSON object only: \"\n",
    "                             \"{'topics': ''}\",\n",
    "        },\n",
    "        \"max_turns\": 1,\n",
    "        \"clear_history\" : False\n",
    "    },\n",
    "    {\n",
    "        \"sender\": customer_proxy_agent,\n",
    "        \"recipient\": customer_engagement_agent,\n",
    "        \"message\": \"Let's find something fun to read.\",\n",
    "        \"max_turns\": 1,\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mOnboarding Personal Information Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Hello, I'm here to help you get started with our product.\n",
      "            Could you tell me your name and location?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding Personal Information Agent):\n",
      "\n",
      "cy\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mOnboarding Personal Information Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Thank you, Cy! Could you please share your location?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding Personal Information Agent):\n",
      "\n",
      "dc\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mOnboarding Topic preference Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Great! Could you tell me what topics you are interested in reading about?\n",
      "Context: \n",
      "```json\n",
      "{\"name\": \"Cy\", \"location\": \"dc\"}\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Onboarding Topic preference Agent):\n",
      "\n",
      "museums\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[34mStarting a new chat....\u001b[0m\n",
      "\u001b[34m\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33mcustomer_proxy_agent\u001b[0m (to Customer Engagement Agent):\n",
      "\n",
      "Let's find something fun to read.\n",
      "Context: \n",
      "```json\n",
      "{\"name\": \"Cy\", \"location\": \"dc\"}\n",
      "```\n",
      "```json\n",
      "{\n",
      "  \"topics\": \"museums\"\n",
      "}\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCustomer Engagement Agent\u001b[0m (to customer_proxy_agent):\n",
      "\n",
      "Hey Cy from DC! üéâ Since you're interested in museums, I've got some fun facts and a joke that'll tickle your curiosity!\n",
      "\n",
      "**Fun Fact 1:** Did you know that the Smithsonian Institution in Washington, D.C. is the largest museum complex in the world? It has over 19 museums, 21 libraries, and a zoo! That‚Äôs a whole lot of knowledge and fun! ü¶Åüìö\n",
      "\n",
      "**Fun Fact 2:** The Louvre Museum in Paris is not only the most visited museum in the world, but it also used to be a royal palace! Imagine living in a place filled with such legendary artwork like the Mona Lisa and the Venus de Milo! üé®üè∞\n",
      "\n",
      "**Museum Joke:** Why did the museum curator break up with the history book?  \n",
      "Because it had too many exes! üòÑüìñ\n",
      "\n",
      "If you ever have a chance to explore more museums, there are always fascinating stories and artifacts waiting for you. Enjoy your adventures in the world of museums, Cy! \n",
      "\n",
      "Now, what‚Äôs your favorite museum or exhibit in DC? I'd love to hear about it! \n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Kick off the parallel, sequential agents\n",
    "from autogen import initiate_chats\n",
    "\n",
    "chat_results = initiate_chats(chats)\n",
    "# chat_results = customer_proxy_agent.initiate_chats(chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length conversation: 3\n",
      "```json\n",
      "{\"name\": \"Cy\", \"location\": \"dc\"}\n",
      "```\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"topics\": \"museums\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "Cy from DC is interested in museums. Fun facts were shared about the Smithsonian Institution and the Louvre Museum, and a museum-related joke was provided. The conversation prompted engagement by asking about Cy's favorite museum or exhibit in DC.\n",
      "\n",
      "\n",
      "{'usage_including_cached_inference': {'total_cost': 0, 'gpt-4o-mini-2024-07-18': {'cost': 0, 'prompt_tokens': 1146, 'completion_tokens': 186, 'total_tokens': 1332}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'gpt-4o-mini-2024-07-18': {'cost': 0, 'prompt_tokens': 822, 'completion_tokens': 134, 'total_tokens': 956}}}\n",
      "\n",
      "\n",
      "{'usage_including_cached_inference': {'total_cost': 0, 'gpt-4o-mini-2024-07-18': {'cost': 0, 'prompt_tokens': 1272, 'completion_tokens': 186, 'total_tokens': 1458}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'gpt-4o-mini-2024-07-18': {'cost': 0, 'prompt_tokens': 1272, 'completion_tokens': 186, 'total_tokens': 1458}}}\n",
      "\n",
      "\n",
      "{'usage_including_cached_inference': {'total_cost': 0, 'gpt-4o-mini-2024-07-18': {'cost': 0, 'prompt_tokens': 2339, 'completion_tokens': 1576, 'total_tokens': 3915}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'gpt-4o-mini-2024-07-18': {'cost': 0, 'prompt_tokens': 2339, 'completion_tokens': 1576, 'total_tokens': 3915}}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "print(f\"Length conversation: {len(chat_results)}\")\n",
    "# View the chats\n",
    "# pprint.pprint(chat_results.chat_history)\n",
    "# Summarize, only if you turned it on during instantiation\n",
    "for chat_result in chat_results:\n",
    "    print(chat_result.summary)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# View token usage\n",
    "for chat_result in chat_results:\n",
    "    print(chat_result.cost)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try again with user input\n",
    "# financial_tasks = [\n",
    "#     \"\"\"What are the current stock prices of NVDA and TESLA, and how is the performance over the past month in terms of percentage change?\"\"\",\n",
    "#     \"\"\"Investigate possible reasons of the stock performance leveraging market news.\"\"\",\n",
    "# ]\n",
    "\n",
    "# writing_tasks = [\"\"\"Develop an engaging blog post using any information provided.\"\"\"]\n",
    "\n",
    "# financial_assistant = autogen.AssistantAgent(\n",
    "#     name=\"Financial_assistant\",\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "# research_assistant = autogen.AssistantAgent(\n",
    "#     name=\"Researcher\",\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "# writer = autogen.AssistantAgent(\n",
    "#     name=\"writer\",\n",
    "#     llm_config=llm_config,\n",
    "#     system_message=\"\"\"\n",
    "#         You are a professional writer, known for\n",
    "#         your insightful and engaging articles.\n",
    "#         You transform complex concepts into compelling narratives.\n",
    "#         Reply \"TERMINATE\" in the end when everything is done.\n",
    "#         \"\"\",\n",
    "# )\n",
    "\n",
    "# user_proxy_auto = autogen.UserProxyAgent(\n",
    "#     name=\"User_Proxy_Auto\",\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "#     code_execution_config={\n",
    "#         \"last_n_messages\": 1,\n",
    "#         \"work_dir\": \"tasks\",\n",
    "#         \"use_docker\": False,\n",
    "#     },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "# )\n",
    "\n",
    "# user_proxy = autogen.UserProxyAgent(\n",
    "#     name=\"User_Proxy\",\n",
    "#     human_input_mode=\"ALWAYS\",  # ask human for input at each step\n",
    "#     is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "#     code_execution_config={\n",
    "#         \"last_n_messages\": 1,\n",
    "#         \"work_dir\": \"tasks\",\n",
    "#         \"use_docker\": False,\n",
    "#     },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "# )\n",
    "\n",
    "\n",
    "# chat_results = autogen.initiate_chats([\n",
    "#     {\n",
    "#         \"sender\": user_proxy_auto,\n",
    "#         \"recipient\": financial_assistant,\n",
    "#         \"message\": financial_tasks[0],\n",
    "#         \"clear_history\": True,\n",
    "#         \"silent\": False,\n",
    "#         \"summary_method\": \"last_msg\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"sender\": user_proxy_auto,\n",
    "#         \"recipient\": research_assistant,\n",
    "#         \"message\": financial_tasks[1],\n",
    "#         \"max_turns\": 2,  # max number of turns for the conversation (added for demo purposes, generally not necessarily needed)\n",
    "#         \"summary_method\": \"reflection_with_llm\",\n",
    "#     },\n",
    "#     {\n",
    "#         \"sender\": user_proxy,\n",
    "#         \"recipient\": writer,\n",
    "#         \"message\": writing_tasks[0],\n",
    "#         \"carryover\": \"I want to include a figure or a table of data in the blogpost.\",  # additional carryover to include to the conversation (added for demo purposes, generally not necessarily needed)\n",
    "#     },\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developer tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length conversation: 10\n",
      "('The conversation highlights the enjoyment of puns and jokes, emphasizing how '\n",
      " 'humor can bring smiles and lightness to life. Both participants share their '\n",
      " 'favorite jokes and appreciate the delivery and cleverness of wordplay, '\n",
      " 'fostering a fun and playful exchange.')\n",
      "{'usage_excluding_cached_inference': {'gpt-4o-mini-2024-07-18': {'completion_tokens': 1799,\n",
      "                                                                 'cost': 0,\n",
      "                                                                 'prompt_tokens': 27138,\n",
      "                                                                 'total_tokens': 28937},\n",
      "                                      'total_cost': 0},\n",
      " 'usage_including_cached_inference': {'gpt-4o-mini-2024-07-18': {'completion_tokens': 2170,\n",
      "                                                                 'cost': 0,\n",
      "                                                                 'prompt_tokens': 28526,\n",
      "                                                                 'total_tokens': 30696},\n",
      "                                      'total_cost': 0}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "print(f\"Length conversation: {len(chat_result.chat_history)}\")\n",
    "# View the chats\n",
    "# pprint.pprint(chat_result.chat_history)\n",
    "# Summarize, only if you turned it on during instantiation\n",
    "pprint.pprint(chat_result.summary)\n",
    "\n",
    "# View token usage\n",
    "pprint.pprint(chat_result.cost)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
