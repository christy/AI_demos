{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single response evaluation using RAGAS\n",
    "\n",
    "https://docs.ragas.io/en/stable/concepts/metrics/available_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ragas: 0.2.14\n"
     ]
    }
   ],
   "source": [
    "# !python -m pip install -U ragas dataset\n",
    "import ragas\n",
    "print(f\"ragas: {ragas.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm: LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\n",
      "embeddings: LangchainEmbeddingsWrapper(embeddings=OpenAIEmbeddings(...))\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries to customize ragas embedding model.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Change the default llm-as-critic LLM.\n",
    "LLM_NAME = \"gpt-4o-mini\" #OpenAI\n",
    "ragas_llm = ragas.llms.llm_factory(model=LLM_NAME)\n",
    "print(f\"llm: {ragas_llm}\")\n",
    "\n",
    "# Change the default embeddings\n",
    "# Initialize OpenAIEmbeddings with the specified model and dimensions\n",
    "lc_embed_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=512)\n",
    "# Wrap the Langchain embeddings model for RAGAS\n",
    "ragas_emb = LangchainEmbeddingsWrapper(embeddings=lc_embed_model)\n",
    "print(f\"embeddings: {ragas_emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN THE ORIGINAL DOCS\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_folder = 'data'\n",
    "\n",
    "# Read the contents of doc1.txt and doc2.txt\n",
    "with open(os.path.join(data_folder, 'doc1.txt'), 'r', encoding='utf-8') as file:\n",
    "    doc1 = file.read()\n",
    "\n",
    "with open(os.path.join(data_folder, 'doc2.txt'), 'r', encoding='utf-8') as file:\n",
    "    doc2 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET THE ORIGINAL PROMPT TEMPLATE\n",
    "\n",
    "prompt_template = \"\"\"You are a legal analyst tasked with performing a detailed comparative analysis of two lengthy legal agreements:\n",
    "Doc1: {doc1}\n",
    "Doc2: {doc2}\n",
    "\n",
    "Analyze what each document says about each topic carefully to identify and present the contextual differences \n",
    "in how each document addresses each topic. The topics to compare are:\n",
    "- Definition of Confidential Information\n",
    "- Permitted Use & Restrictions\n",
    "- Data Security\n",
    "\n",
    "Output JSON summarizing the contextual differences between Doc1 and Doc2 for each topic.\n",
    "JSON keys: \n",
    "topic, summary, doc1_context, doc2_context\n",
    "\n",
    "summary: \n",
    "Provide a concise summary of the key contextual difference between Doc1 and Doc2 \n",
    "for the specific feature. Focus on the *meaningful distinction* and its *practical implications*.\n",
    "\n",
    "doc1_context: \n",
    "Quote the *relevant text excerpt* from Doc1 that pertains to the feature. \n",
    "**Within this quoted text, use bold markdown formatting to highlight the specific words \n",
    "or phrases that are different or absent compared to the corresponding clause in Doc2.**\n",
    "\n",
    "doc2_context: \n",
    "Quote the *corresponding text excerpt* from Doc2 that addresses the same feature. \n",
    "**Within this quoted text, use bold markdown formatting to highlight the specific words \n",
    "or phrases that are different or absent compared to the clause in Doc1.**\n",
    "\n",
    "Use bold markdown to highlight the differing text within the \"doc1_context\" and \"doc2_context\" columns as described above.\n",
    "Make sure you highlight in bold for each row, only text differences, to make it easier for the user to see the differences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>context length</th>\n",
       "      <th>time to response</th>\n",
       "      <th>correct structure</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta-Llama-3.3-70B-Instruct</td>\n",
       "      <td>128K</td>\n",
       "      <td>3s 484ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>topic='Definition of Confidential Information'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepSeek-R1 w/2K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>1m 46s 770ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information**: Document A uses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSeek-R1 w/4K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>49s 450ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepSeek-R1 w/7K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>12s 975ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>128K</td>\n",
       "      <td>5s 910ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>“topic\": \"Definition of Confidential Informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...</td>\n",
       "      <td>128K</td>\n",
       "      <td>12s 940ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information** | Document A def...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model context length  \\\n",
       "0                        Meta-Llama-3.3-70B-Instruct           128K   \n",
       "1                        DeepSeek-R1 w/2K chunk size             8K   \n",
       "2                        DeepSeek-R1 w/4K chunk size             8K   \n",
       "3                        DeepSeek-R1 w/7K chunk size             8K   \n",
       "4                      DeepSeek-R1-Distill-Llama-70B           128K   \n",
       "5  Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...           128K   \n",
       "\n",
       "  time to response correct structure  \\\n",
       "0         3s 484ms               Yes   \n",
       "1     1m 46s 770ms               Yes   \n",
       "2        49s 450ms               Yes   \n",
       "3        12s 975ms               Yes   \n",
       "4         5s 910ms               Yes   \n",
       "5        12s 940ms               Yes   \n",
       "\n",
       "                                              answer  \n",
       "0  topic='Definition of Confidential Information'...  \n",
       "1  **Confidential Information**: Document A uses ...  \n",
       "2  \"topic\": \"Definition of Confidential Informati...  \n",
       "3  \"topic\": \"Definition of Confidential Informati...  \n",
       "4  “topic\": \"Definition of Confidential Informati...  \n",
       "5   **Confidential Information** | Document A def...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the current working directory.\n",
    "cwd = os.getcwd()\n",
    "relative_path = '/evals/sambanova_example_deepseekr1_context10k.csv'\n",
    "file_path = cwd + relative_path\n",
    "# print(f\"file_path: {file_path}\")\n",
    "\n",
    "# Read LLM answers to evaluate from a CSV file.\n",
    "eval_df = pd.read_csv(file_path, header=0, skip_blank_lines=True)\n",
    "eval_df = eval_df.iloc[:, 0:5].copy()  # keep only 1st 5 columns\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Summarization scorer for every model and append to eval df\n",
    "\n",
    "[doc link](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/summarization_score/#summarization-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47197409790326283,\n",
       " 0.47293189247714834,\n",
       " 0.47327544922647685,\n",
       " 0.4696628979532347,\n",
       " 0.4695587898473776,\n",
       " 0.4759926707893477]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import SummarizationScore\n",
    "\n",
    "# https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/summarization_score/#summarization-score\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Loop through each answer in eval_df.answer\n",
    "for answer in eval_df.answer:\n",
    "    # Assemble a single turn summarization data point\n",
    "    sample = SingleTurnSample(\n",
    "        response=answer,  # Use the current answer in the loop\n",
    "        reference_contexts=[doc1, doc2]\n",
    "    )\n",
    "    scorer = SummarizationScore(llm=ragas_llm)\n",
    "    score = await scorer.single_turn_ascore(sample)  # Get the score for the current sample\n",
    "    scores.append(score)  # Append the score to the list\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>context length</th>\n",
       "      <th>time to response</th>\n",
       "      <th>correct structure</th>\n",
       "      <th>answer</th>\n",
       "      <th>summarization_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta-Llama-3.3-70B-Instruct</td>\n",
       "      <td>128K</td>\n",
       "      <td>3s 484ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>topic='Definition of Confidential Information'...</td>\n",
       "      <td>0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepSeek-R1 w/2K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>1m 46s 770ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information**: Document A uses ...</td>\n",
       "      <td>0.473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSeek-R1 w/4K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>49s 450ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepSeek-R1 w/7K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>12s 975ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>128K</td>\n",
       "      <td>5s 910ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>“topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...</td>\n",
       "      <td>128K</td>\n",
       "      <td>12s 940ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information** | Document A def...</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model context length  \\\n",
       "0                        Meta-Llama-3.3-70B-Instruct           128K   \n",
       "1                        DeepSeek-R1 w/2K chunk size             8K   \n",
       "2                        DeepSeek-R1 w/4K chunk size             8K   \n",
       "3                        DeepSeek-R1 w/7K chunk size             8K   \n",
       "4                      DeepSeek-R1-Distill-Llama-70B           128K   \n",
       "5  Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...           128K   \n",
       "\n",
       "  time to response correct structure  \\\n",
       "0         3s 484ms               Yes   \n",
       "1     1m 46s 770ms               Yes   \n",
       "2        49s 450ms               Yes   \n",
       "3        12s 975ms               Yes   \n",
       "4         5s 910ms               Yes   \n",
       "5        12s 940ms               Yes   \n",
       "\n",
       "                                              answer  summarization_score  \n",
       "0  topic='Definition of Confidential Information'...                0.472  \n",
       "1  **Confidential Information**: Document A uses ...                0.473  \n",
       "2  \"topic\": \"Definition of Confidential Informati...                0.473  \n",
       "3  \"topic\": \"Definition of Confidential Informati...                0.470  \n",
       "4  “topic\": \"Definition of Confidential Informati...                0.470  \n",
       "5   **Confidential Information** | Document A def...                0.476  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append scores to eval_df\n",
    "eval_df['summarization_score'] = np.round(scores, 3)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36, 0.34, 0.5, 0.0, 0.68, 0.84]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics._factual_correctness import FactualCorrectness\n",
    "# https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/factual_correctness\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Loop through each answer in eval_df.answer\n",
    "for answer in eval_df.answer:\n",
    "    # Assemble a single turn summarization data point\n",
    "    sample = SingleTurnSample(\n",
    "        response=answer,  # Use the current answer in the loop\n",
    "        reference=doc1 + \" \" + doc2\n",
    "    )\n",
    "    scorer = FactualCorrectness(\n",
    "        llm=ragas_llm, \n",
    "        mode=\"precision\", atomicity=\"low\")\n",
    "    score = await scorer.single_turn_ascore(sample)  # Get the score for the current sample\n",
    "    scores.append(score)  # Append the score to the list\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>context length</th>\n",
       "      <th>time to response</th>\n",
       "      <th>correct structure</th>\n",
       "      <th>answer</th>\n",
       "      <th>summarization_score</th>\n",
       "      <th>correctness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta-Llama-3.3-70B-Instruct</td>\n",
       "      <td>128K</td>\n",
       "      <td>3s 484ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>topic='Definition of Confidential Information'...</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepSeek-R1 w/2K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>1m 46s 770ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information**: Document A uses ...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSeek-R1 w/4K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>49s 450ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepSeek-R1 w/7K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>12s 975ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>128K</td>\n",
       "      <td>5s 910ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>“topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...</td>\n",
       "      <td>128K</td>\n",
       "      <td>12s 940ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information** | Document A def...</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model context length  \\\n",
       "0                        Meta-Llama-3.3-70B-Instruct           128K   \n",
       "1                        DeepSeek-R1 w/2K chunk size             8K   \n",
       "2                        DeepSeek-R1 w/4K chunk size             8K   \n",
       "3                        DeepSeek-R1 w/7K chunk size             8K   \n",
       "4                      DeepSeek-R1-Distill-Llama-70B           128K   \n",
       "5  Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...           128K   \n",
       "\n",
       "  time to response correct structure  \\\n",
       "0         3s 484ms               Yes   \n",
       "1     1m 46s 770ms               Yes   \n",
       "2        49s 450ms               Yes   \n",
       "3        12s 975ms               Yes   \n",
       "4         5s 910ms               Yes   \n",
       "5        12s 940ms               Yes   \n",
       "\n",
       "                                              answer  summarization_score  \\\n",
       "0  topic='Definition of Confidential Information'...                0.472   \n",
       "1  **Confidential Information**: Document A uses ...                0.473   \n",
       "2  \"topic\": \"Definition of Confidential Informati...                0.473   \n",
       "3  \"topic\": \"Definition of Confidential Informati...                0.470   \n",
       "4  “topic\": \"Definition of Confidential Informati...                0.470   \n",
       "5   **Confidential Information** | Document A def...                0.476   \n",
       "\n",
       "   correctness_score  \n",
       "0               0.36  \n",
       "1               0.34  \n",
       "2               0.50  \n",
       "3               0.00  \n",
       "4               0.68  \n",
       "5               0.84  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append scores to eval_df\n",
    "eval_df['correctness_score'] = scores\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas.metrics import AnswerAccuracy\n",
    "# # NVIDIA contribution\n",
    "# # https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/nvidia_metrics/#answer-accuracy\n",
    "\n",
    "# scores = []\n",
    "\n",
    "# # Loop through each answer in eval_df.answer\n",
    "# for answer in eval_df.answer:\n",
    "#     # Assemble a single turn summarization data point\n",
    "#     sample = SingleTurnSample(\n",
    "#         user_input=prompt_template,\n",
    "#         response=answer,\n",
    "#         reference=doc1 + \" \" + doc2\n",
    "#     )\n",
    "#     scorer = AnswerAccuracy(llm=ragas_llm)\n",
    "#     score = await scorer.single_turn_ascore(sample)  # Get the score for the current sample\n",
    "#     scores.append(score)  # Append the score to the list\n",
    "\n",
    "#     # Free account rate limit is 3 requests per minute\n",
    "#     # Wait for 1 minute\n",
    "#     time.sleep(60)\n",
    "\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Append scores to eval_df\n",
    "# eval_df['nvidia_accuracy_score'] = scores\n",
    "# eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>context length</th>\n",
       "      <th>time to response</th>\n",
       "      <th>correct structure</th>\n",
       "      <th>answer</th>\n",
       "      <th>summarization_score</th>\n",
       "      <th>correctness_score</th>\n",
       "      <th>mean_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta-Llama-3.3-70B-Instruct</td>\n",
       "      <td>128K</td>\n",
       "      <td>3s 484ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>topic='Definition of Confidential Information'...</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.4160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepSeek-R1 w/2K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>1m 46s 770ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information**: Document A uses ...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.4065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSeek-R1 w/4K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>49s 450ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.4865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepSeek-R1 w/7K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>12s 975ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>128K</td>\n",
       "      <td>5s 910ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>“topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.5750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...</td>\n",
       "      <td>128K</td>\n",
       "      <td>12s 940ms</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information** | Document A def...</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.6580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model context length  \\\n",
       "0                        Meta-Llama-3.3-70B-Instruct           128K   \n",
       "1                        DeepSeek-R1 w/2K chunk size             8K   \n",
       "2                        DeepSeek-R1 w/4K chunk size             8K   \n",
       "3                        DeepSeek-R1 w/7K chunk size             8K   \n",
       "4                      DeepSeek-R1-Distill-Llama-70B           128K   \n",
       "5  Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...           128K   \n",
       "\n",
       "  time to response correct structure  \\\n",
       "0         3s 484ms               Yes   \n",
       "1     1m 46s 770ms               Yes   \n",
       "2        49s 450ms               Yes   \n",
       "3        12s 975ms               Yes   \n",
       "4         5s 910ms               Yes   \n",
       "5        12s 940ms               Yes   \n",
       "\n",
       "                                              answer  summarization_score  \\\n",
       "0  topic='Definition of Confidential Information'...                0.472   \n",
       "1  **Confidential Information**: Document A uses ...                0.473   \n",
       "2  \"topic\": \"Definition of Confidential Informati...                0.473   \n",
       "3  \"topic\": \"Definition of Confidential Informati...                0.470   \n",
       "4  “topic\": \"Definition of Confidential Informati...                0.470   \n",
       "5   **Confidential Information** | Document A def...                0.476   \n",
       "\n",
       "   correctness_score  mean_score  \n",
       "0               0.36      0.4160  \n",
       "1               0.34      0.4065  \n",
       "2               0.50      0.4865  \n",
       "3               0.00      0.2350  \n",
       "4               0.68      0.5750  \n",
       "5               0.84      0.6580  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate a mean score\n",
    "eval_df['mean_score'] = \\\n",
    "    eval_df[['summarization_score', 'correctness_score']]\\\n",
    "    .mean(axis=1)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>context length</th>\n",
       "      <th>time to response</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>percent_improvement</th>\n",
       "      <th>correct structure</th>\n",
       "      <th>answer</th>\n",
       "      <th>summarization_score</th>\n",
       "      <th>correctness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...</td>\n",
       "      <td>128K</td>\n",
       "      <td>12s 940ms</td>\n",
       "      <td>0.6580</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information** | Document A def...</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeepSeek-R1-Distill-Llama-70B</td>\n",
       "      <td>128K</td>\n",
       "      <td>5s 910ms</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>59.130435</td>\n",
       "      <td>Yes</td>\n",
       "      <td>“topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepSeek-R1 w/4K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>49s 450ms</td>\n",
       "      <td>0.4865</td>\n",
       "      <td>51.695786</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meta-Llama-3.3-70B-Instruct</td>\n",
       "      <td>128K</td>\n",
       "      <td>3s 484ms</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>43.509615</td>\n",
       "      <td>Yes</td>\n",
       "      <td>topic='Definition of Confidential Information'...</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeepSeek-R1 w/2K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>1m 46s 770ms</td>\n",
       "      <td>0.4065</td>\n",
       "      <td>42.189422</td>\n",
       "      <td>Yes</td>\n",
       "      <td>**Confidential Information**: Document A uses ...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DeepSeek-R1 w/7K chunk size</td>\n",
       "      <td>8K</td>\n",
       "      <td>12s 975ms</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Yes</td>\n",
       "      <td>\"topic\": \"Definition of Confidential Informati...</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model context length  \\\n",
       "0  Together.ai’s deepseek-ai/DeepSeek-R1-Distill-...           128K   \n",
       "1                      DeepSeek-R1-Distill-Llama-70B           128K   \n",
       "2                        DeepSeek-R1 w/4K chunk size             8K   \n",
       "3                        Meta-Llama-3.3-70B-Instruct           128K   \n",
       "4                        DeepSeek-R1 w/2K chunk size             8K   \n",
       "5                        DeepSeek-R1 w/7K chunk size             8K   \n",
       "\n",
       "  time to response  mean_score  percent_improvement correct structure  \\\n",
       "0        12s 940ms      0.6580            64.285714               Yes   \n",
       "1         5s 910ms      0.5750            59.130435               Yes   \n",
       "2        49s 450ms      0.4865            51.695786               Yes   \n",
       "3         3s 484ms      0.4160            43.509615               Yes   \n",
       "4     1m 46s 770ms      0.4065            42.189422               Yes   \n",
       "5        12s 975ms      0.2350             0.000000               Yes   \n",
       "\n",
       "                                              answer  summarization_score  \\\n",
       "0   **Confidential Information** | Document A def...                0.476   \n",
       "1  “topic\": \"Definition of Confidential Informati...                0.470   \n",
       "2  \"topic\": \"Definition of Confidential Informati...                0.473   \n",
       "3  topic='Definition of Confidential Information'...                0.472   \n",
       "4  **Confidential Information**: Document A uses ...                0.473   \n",
       "5  \"topic\": \"Definition of Confidential Informati...                0.470   \n",
       "\n",
       "   correctness_score  \n",
       "0               0.84  \n",
       "1               0.68  \n",
       "2               0.50  \n",
       "3               0.36  \n",
       "4               0.34  \n",
       "5               0.00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## REPORTING\n",
    "\n",
    "def calculate_percent_lift(scores):\n",
    "    # Calculate how much better each score is than worst score\n",
    "    percent_better = np.abs(scores - scores[5]) \\\n",
    "                         / scores * 100\n",
    "    return percent_better\n",
    "\n",
    "## ACCURACY\n",
    "# Sort from highest to lowest mean accuracy score\n",
    "sorted_df = eval_df.sort_values(by=eval_df.columns[-1], ascending=False).reset_index(drop=True)\n",
    "# Just the sorted mean scores\n",
    "scores = sorted_df.mean_score\n",
    "percent_better = calculate_percent_lift(scores)\n",
    "# Add percents to eval_df\n",
    "sorted_df['percent_improvement'] = percent_better\n",
    "# Reorder scores columns\n",
    "sorted_df = sorted_df.iloc[:, [0,1,2,7,8,3,4,5,6]]\n",
    "\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS MOST ACCURATE  (all models hosted by SambaNova, except 1 hosted by Together.ai)\n",
    "\n",
    "|rank<br> model\t| context length | TTR | mean_score | percent more<br> accurate |\n",
    "|--|----------------|-----|------------|---------------------|\n",
    "|1 | Together.ai’s DeepSeek-R1-Distill-Llama-70B | 128K| 13s | 0.6580 | 64.285714 |\n",
    "|2 | DeepSeek-R1-Distill-Llama-70B | 128K | 6s | 0.5750 | 59.130435 |\n",
    "|3 | DeepSeek-R1 w/4K chunk size | 8K | 49s | 0.4865 | 51.695786 |\n",
    "|4 | Meta-Llama-3.3-70B-Instruct | 128K | 3s | 0.4160 | 43.509615 |\n",
    "5 | DeepSeek-R1 w/2K chunk size | 8K | 1m 47s\t| 0.4065 | 42.189422 |\n",
    "6 | DeepSeek-R1 w/7K chunk size | 8K | 13s | 0.2350 | 0.000000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS FASTEST (all models hosted by SambaNova, except 1 hosted by Together.ai)\n",
    "\n",
    "|rank <br>(accuracy) <br>model | context length | TTR | mean_score | percent <br>faster |\n",
    "|-----------|----------------|-----|------------|---------------------|\n",
    "|2 | DeepSeek-R1-Distill-Llama-70B | 128K | 910ms | 0.5750 | 98% |\n",
    "|1 | Together.ai’s DeepSeek-R1-Distill-Llama-70B | 128K | 1300ms | 0.6580 | 98% |\n",
    "|4 | Meta-Llama-3.3-70B-Instruct | 128K | 3484ms | 0.4160 | 94% |\n",
    "|6 | DeepSeek-R1 w/7K chunk size | 8K | 12975ms | 0.2350 | 78% |\n",
    "|3 | DeepSeek-R1 w/4K chunk size | 8K | 49045ms | 0.4865 | 18% |\n",
    "|5 | DeepSeek-R1 w/2K chunk size | 8K | 60046ms\t| 0.4065 | 0% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1832095393531626"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(49045-60046)/60046"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
