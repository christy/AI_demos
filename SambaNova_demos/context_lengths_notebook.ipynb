{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Long Context Lengths\n",
    "\n",
    "- Prompt 128K question to Llama 3.3 11B which has 128K context length\n",
    "- Prompt 128K question to DeepSeek-R1 model which has 8K context length - error!\n",
    "- Naive Recursive Text Splitting + 1 extra combining prompt to DeepSeek-R1\n",
    "- Prompt distilled DeepSeek-R1 model with 128K context length\n",
    "- Prompt YaRn (new RoPE) fine tuned 32K model on vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christy/Documents/py312/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in SambaStudioEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SambaNova chat uses OpenAI API\n",
    "# https://github.com/sambanova/ai-starter-kit/blob/main/quickstart/README.md\n",
    "# from openai import OpenAI\n",
    "import os, tiktoken, time, pprint\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# SambaNova LangChain integration to chain structured output\n",
    "# !python3 -m pip install langchain-sambanova\n",
    "from langchain_sambanova import ChatSambaNovaCloud\n",
    "\n",
    "# Authenticate into SambaNova Cloud using API key\n",
    "api_key = os.environ.get(\"SAMBANOVA_API_KEY\")\n",
    "\n",
    "# Chat mode\n",
    "streaming = False  #streaming turned off\n",
    "\n",
    "# List all models\n",
    "# https://cloud.sambanova.ai > Playground\n",
    "model=\"Meta-Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# Wrap LangChain model for SambaNova\n",
    "llm = ChatSambaNovaCloud(\n",
    "    model=model,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "# topic_desc = \"\"\" \n",
    "# Concise summary of the key contextual difference between Doc1 and Doc2 for the specific topic. \n",
    "# Focus on the meaningful distinction and its practical implications.\n",
    "# \"\"\"\n",
    "\n",
    "# doc1_context_desc = \"\"\"Relevant text excerpts from Doc1 that pertain to the topic. \n",
    "# Within this quoted text, use bold markdown formatting to highlight\n",
    "# or phrases that are different or absent compared to the corresponding clauses in Doc2.\n",
    "# \"\"\"\n",
    "\n",
    "# doc2_context_desc = \"\"\"Corresponding text excerpts from Doc2 that address the same topic. \n",
    "# Within this quoted text, use bold markdown formatting to highlight\n",
    "# or phrases that are different or absent compared to the clause in Doc1.\n",
    "# \"\"\"\n",
    "\n",
    "# Pydantic class that represents a single difference between 2 docs.\n",
    "# Use OpenAI tool schema (JSON)\n",
    "class Difference(BaseModel):\n",
    "    topic: str = Field(..., description=\"Topic\")\n",
    "    summary: str = Field(..., description=\"Summary of differences per topic\")\n",
    "    doc1_context: str = Field(..., description=\"Context sentences from Document 1 with all and only differing words highlighted.\")\n",
    "    doc2_context: str = Field(..., description=\"Context sentences from Document 2 with all and only differing words highlighted.\")\n",
    "\n",
    "# Pydantic class containing a list of Difference objects.\n",
    "class Differences(BaseModel):\n",
    "    differences: List[Difference] = Field(..., description=\"A list of differences between two documents.\")\n",
    "\n",
    "# Function to count tokens\n",
    "def count_openai_tokens(prompt):\n",
    "    # Count how many OpenAI tokens are used by the prompt\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Encode the prompt to get the tokens\n",
    "    tokens = encoding.encode(prompt)\n",
    "\n",
    "    # Count the number of tokens\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # Print the number of tokens\n",
    "    print(f\"Number of tokens in the prompt: {num_tokens}\")\n",
    "\n",
    "    return num_tokens\n",
    "\n",
    "# Function to display markdown of JSON \"differences\"\n",
    "def display_differences_as_markdown(differences):\n",
    "    \"\"\"\n",
    "    Function to display differences in a markdown table format.\n",
    "    \n",
    "    Args:\n",
    "        differences (Differences): An instance of the Differences class containing a list of Difference objects.\n",
    "    \"\"\"\n",
    "    # Start the markdown table\n",
    "    markdown_table = \"| Topic | Summary Differences | Doc1 Context | Doc2 Context |\\n\"\n",
    "    markdown_table += \"|-------|--------------|--------------|--------------\\n\"\n",
    "    \n",
    "    # Loop through each difference and format it into the table\n",
    "    for diff in differences.differences:\n",
    "        markdown_table += f\"| {diff.topic} | {diff.summary} | {diff.doc1_context} | {diff.doc2_context} |\\n\"\n",
    "    \n",
    "    # Display the markdown table\n",
    "    display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example documents\n",
    "\n",
    "- Prompt token count = 10367  (10K prompt for 8K context window)\n",
    "- Replace doc1, with all text from https://sambanova.ai/cloud-end-user-license-agreement\n",
    "- Replace doc2, with all text from https://developers.google.com/terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the data folder\n",
    "data_folder = 'data'\n",
    "\n",
    "# Read the contents of doc1.txt and doc2.txt\n",
    "with open(os.path.join(data_folder, 'doc1.txt'), 'r', encoding='utf-8') as file:\n",
    "    doc1 = file.read()\n",
    "\n",
    "with open(os.path.join(data_folder, 'doc2.txt'), 'r', encoding='utf-8') as file:\n",
    "    doc2 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 10057\n"
     ]
    }
   ],
   "source": [
    "# doc1 = \"The quick brown fox jumped over the lazy dog\"\n",
    "# doc2 = \"The slow brown fox jumped over the lazy cat\"\n",
    "\n",
    "prompt_template = \"\"\"You are a legal analyst tasked with performing a detailed comparative analysis of two lengthy legal agreements:\n",
    "Doc1: {doc1}\n",
    "Doc2: {doc2}\n",
    "\n",
    "Analyze what each document says about each topic carefully to identify and present the contextual differences \n",
    "in how each document addresses each topic. The topics to compare are:\n",
    "- Definition of Confidential Information\n",
    "- Permitted Use & Restrictions\n",
    "- Data Security\n",
    "\n",
    "Output JSON summarizing the contextual differences between Doc1 and Doc2 for each topic.\n",
    "JSON keys: \n",
    "topic, summary, doc1_context, doc2_context\n",
    "\n",
    "summary: \n",
    "Provide a concise summary of the key contextual difference between Doc1 and Doc2 \n",
    "for the specific feature. Focus on the *meaningful distinction* and its *practical implications*.\n",
    "\n",
    "doc1_context: \n",
    "Quote the *relevant text excerpt* from Doc1 that pertains to the feature. \n",
    "**Within this quoted text, use bold markdown formatting to highlight the specific words \n",
    "or phrases that are different or absent compared to the corresponding clause in Doc2.**\n",
    "\n",
    "doc2_context: \n",
    "Quote the *corresponding text excerpt* from Doc2 that addresses the same feature. \n",
    "**Within this quoted text, use bold markdown formatting to highlight the specific words \n",
    "or phrases that are different or absent compared to the clause in Doc1.**\n",
    "\n",
    "Use bold markdown to highlight the differing text within the \"doc1_context\" and \"doc2_context\" columns as described above.\n",
    "Make sure you highlight in bold for each row, only text differences, to make it easier for the user to see the differences.\n",
    "\"\"\"\n",
    "\n",
    "# Assemble the prompt\n",
    "prompt = prompt_template.format(\n",
    "    doc1=doc1,\n",
    "    doc2=doc2)\n",
    "\n",
    "# Print number of tokens in the prompt\n",
    "num_tokens = count_openai_tokens(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Llama-3.3-70B-Instruct\n",
    "128K Context Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 3.4840099811553955 seconds for model: Meta-Llama-3.3-70B-Instruct\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Topic | Summary Differences | Doc1 Context | Doc2 Context |\n",
       "|-------|--------------|--------------|--------------\n",
       "| Definition of Confidential Information | Doc1 defines Confidential Information as all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure. In contrast, Doc2 does not provide a clear definition of Confidential Information, but rather refers to it as 'confidential matters' and describes the obligations related to it. | **Confidential Information** shall mean all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure. | Our communications to you and our APIs may contain **Google confidential information**. Google confidential information includes any materials, communications, and information that are marked confidential or that would normally be considered confidential under the circumstances. |\n",
       "| Permitted Use & Restrictions | Doc1 grants a non-exclusive, non-transferable, limited license to use the software embodied in the Service, while Doc2 allows use of the APIs, but with specific restrictions and requirements, such as complying with applicable law and not using the APIs to encourage or promote illegal activity. | Subject to and conditioned upon Customer’s and its Users’ strict compliance with all terms and conditions set forth in this Agreement and the Additional Documents, SambaNova hereby grants to Customer a **non-exclusive, non-transferable, limited license** during the Term to use the software embodied in the Service. | You will comply with all applicable law, regulation, and third party rights (including without limitation laws regarding the import or export of data or software, privacy, and local laws). You will not use the APIs to **encourage or promote illegal activity or violation of third party rights**. |\n",
       "| Data Security | Doc1 requires the implementation and use of reasonable and appropriate measures to help secure the Service, while Doc2 requires the use of commercially reasonable efforts to protect user information collected by the API Client. | You must implement and use **reasonable and appropriate measures** to help secure the Service by you and your Users. | You will use **commercially reasonable efforts** to protect user information collected by your API Client, including personal data, from unauthorized access or use. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chat completion using the specified model.\n",
    "\n",
    "# List all models\n",
    "# https://cloud.sambanova.ai > Playground\n",
    "model=\"Meta-Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# Wrap LangChain model for SambaNova\n",
    "llm = ChatSambaNovaCloud(\n",
    "    model=model,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    ")\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Chat with SambaNova chat completion chain to structured output.\n",
    "# Ask LLM to return structured output conforming to Differences Pydantic class.\n",
    "# That is, return a list of JSON Difference elements.\n",
    "structured_llm = llm.with_structured_output(Differences)\n",
    "\n",
    "# Invoke the model-to-structured-output chain using a prompt.\n",
    "completion = structured_llm.invoke(prompt)\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "response_time = end_time - start_time  # Calculate response time\n",
    "print(f\"Response time: {response_time} seconds for model: {model}\")\n",
    "\n",
    "# # Loop through list of Difference JSON elements.\n",
    "# for diff in completion.differences:\n",
    "#     pprint.pprint(f\"topic: {diff.topic}\")\n",
    "#     pprint.pprint(f\"Doc1: {diff.doc1_context}\")\n",
    "#     pprint.pprint(f\"Doc2: {diff.doc2_context}\") \n",
    "\n",
    "# Render the structured completion JSON as markdown table\n",
    "display_differences_as_markdown(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differences=[Difference(topic='Definition of Confidential Information', summary=\"Doc1 defines Confidential Information as all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure. In contrast, Doc2 does not provide a clear definition of Confidential Information, but rather refers to it as 'confidential matters' and describes the obligations related to it.\", doc1_context='**Confidential Information** shall mean all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure.', doc2_context='Our communications to you and our APIs may contain **Google confidential information**. Google confidential information includes any materials, communications, and information that are marked confidential or that would normally be considered confidential under the circumstances.'), Difference(topic='Permitted Use & Restrictions', summary='Doc1 grants a non-exclusive, non-transferable, limited license to use the software embodied in the Service, while Doc2 allows use of the APIs, but with specific restrictions and requirements, such as complying with applicable law and not using the APIs to encourage or promote illegal activity.', doc1_context='Subject to and conditioned upon Customer’s and its Users’ strict compliance with all terms and conditions set forth in this Agreement and the Additional Documents, SambaNova hereby grants to Customer a **non-exclusive, non-transferable, limited license** during the Term to use the software embodied in the Service.', doc2_context='You will comply with all applicable law, regulation, and third party rights (including without limitation laws regarding the import or export of data or software, privacy, and local laws). You will not use the APIs to **encourage or promote illegal activity or violation of third party rights**.'), Difference(topic='Data Security', summary='Doc1 requires the implementation and use of reasonable and appropriate measures to help secure the Service, while Doc2 requires the use of commercially reasonable efforts to protect user information collected by the API Client.', doc1_context='You must implement and use **reasonable and appropriate measures** to help secure the Service by you and your Users.', doc2_context='You will use **commercially reasonable efforts** to protect user information collected by your API Client, including personal data, from unauthorized access or use.')]\n"
     ]
    }
   ],
   "source": [
    "print(completion)  # for copy/paste to eval table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek-R1\n",
    "8K Context Length\n",
    "\n",
    "The example input docs have token length=10540. <br>\n",
    "If you just submit the chat completion, you'll get error message: <br>\n",
    "\n",
    "`BadRequestError: The maximum context length of DeepSeek-R1 is 8192. However, answering your request will take 10541 tokens. Please reduce the length of the messages.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "('Sambanova /complete call failed with status code 400.', 'The maximum context length of DeepSeek-R1 is 8192. However, answering your request will take 11082 tokens. Please reduce the length of the messages or the specified max_completion_tokens value.\\n.')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(Differences)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Invoke the model-to-structured-output chain using a prompt.\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# End timing\u001b[39;00m\n\u001b[1;32m     24\u001b[0m response_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# Calculate response time\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/py312/lib/python3.12/site-packages/langchain_core/runnables/base.py:3027\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3025\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3027\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3029\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Documents/py312/lib/python3.12/site-packages/langchain_core/runnables/base.py:5365\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5360\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5361\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5362\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5363\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5364\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5366\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/py312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:285\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    282\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    284\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 285\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/Documents/py312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:861\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    855\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    859\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    860\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/py312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:691\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 691\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m         )\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/py312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:926\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 926\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    930\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/py312/lib/python3.12/site-packages/langchain_sambanova/chat_models.py:1000\u001b[0m, in \u001b[0;36mChatSambaNovaCloud._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    999\u001b[0m messages_dicts \u001b[38;5;241m=\u001b[39m _create_message_dicts(messages)\n\u001b[0;32m-> 1000\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(response)\n\u001b[1;32m   1002\u001b[0m generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m   1003\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   1004\u001b[0m     generation_info\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish_reason\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mresponse_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish_reason\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1006\u001b[0m     },\n\u001b[1;32m   1007\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/py312/lib/python3.12/site-packages/langchain_sambanova/chat_models.py:771\u001b[0m, in \u001b[0;36mChatSambaNovaCloud._handle_request\u001b[0;34m(self, messages_dicts, stop, streaming, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m response \u001b[38;5;241m=\u001b[39m http_session\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msambanova_url,\n\u001b[1;32m    762\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    768\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstreaming,\n\u001b[1;32m    769\u001b[0m )\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    772\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSambanova /complete call failed with status code \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    775\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ('Sambanova /complete call failed with status code 400.', 'The maximum context length of DeepSeek-R1 is 8192. However, answering your request will take 11082 tokens. Please reduce the length of the messages or the specified max_completion_tokens value.\\n.')"
     ]
    }
   ],
   "source": [
    "# Chat completion using the specified model.\n",
    "\n",
    "# List all models\n",
    "# https://cloud.sambanova.ai > Playground\n",
    "model = \"DeepSeek-R1\"\n",
    "\n",
    "# Wrap LangChain model for SambaNova\n",
    "llm = ChatSambaNovaCloud(\n",
    "    model=model,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    ")\n",
    "\n",
    "# start_time = time.time()  # Start timing\n",
    "\n",
    "# # Chat with SambaNova chat completion chain to structured output.\n",
    "# structured_llm = llm.with_structured_output(Differences)\n",
    "\n",
    "# # Invoke the model-to-structured-output chain using a prompt.\n",
    "# completion = structured_llm.invoke(prompt)\n",
    "\n",
    "# end_time = time.time()  # End timing\n",
    "# response_time = end_time - start_time  # Calculate response time\n",
    "# print(f\"Response time: {response_time} seconds for model: {model}\")\n",
    "\n",
    "# failed with status code 400.', 'The maximum context length of DeepSeek-R1 is 8192. \n",
    "# However, answering your request will take 11082 tokens. Please reduce the length of \n",
    "# the messages or the specified max_completion_tokens value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk texts before sending prompts to DeepSeek-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Extract json from chat completion\n",
    "def extract_and_display_output(completion,\n",
    "                               structured_output_start = \"```json\\n[\\n  \", \n",
    "                               print_output=True):\n",
    "\n",
    "    try:\n",
    "        # Assuming `completion` is the response from the OpenAI API\n",
    "        diff_dict = completion.__dict__\n",
    "        # Extract answer from the response\n",
    "        response_content = diff_dict['choices'][0].message.content\n",
    "    except:\n",
    "        response_content = completion\n",
    "\n",
    "    # Extract structured part only from the answer\n",
    "    if structured_output_start in response_content:\n",
    "        # Extract the part of the answer starting from the structured output\n",
    "        structured_output = response_content.split(structured_output_start)[1].strip()\n",
    "\n",
    "        # Remove trailing extra characters\n",
    "        structured_output = structured_output.split(']\\n')[0].strip()\n",
    "\n",
    "        # Add beginning and ending []\n",
    "        structured_output = \"[\" + structured_output + \"]\"\n",
    "    else:\n",
    "        structured_output = response_content  # Fallback if the tag is not found\"\n",
    "    # debugging print\n",
    "    print(structured_output)\n",
    "\n",
    "    # Return raw markdown text\n",
    "    return structured_output\n",
    "\n",
    "# Display json as a markdown table\n",
    "def display_json_as_markdown(json_string):\n",
    "    \"\"\"\n",
    "    Function to display JSON data in a markdown table format.\n",
    "    \n",
    "    Args:\n",
    "        json_string (str): A JSON string containing the data to display.\n",
    "    \"\"\"\n",
    "    # Load the JSON string into a Python object\n",
    "    try:\n",
    "        json_data = json.loads(json_string)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON string provided.\")\n",
    "        return\n",
    "\n",
    "    # Check if json_data is not empty\n",
    "    if not json_data:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    # Extract headers from the first dictionary\n",
    "    headers = json_data[0].keys()\n",
    "    \n",
    "    # Start the markdown table\n",
    "    markdown_table = \"| \" + \" | \".join(headers) + \" |\\n\"\n",
    "    markdown_table += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n",
    "    \n",
    "    # Loop through each dictionary and format it into the table\n",
    "    for entry in json_data:\n",
    "        row = \" | \".join(str(entry[header]) for header in headers)\n",
    "        markdown_table += f\"| {row} |\\n\"\n",
    "    \n",
    "    # Display the markdown table\n",
    "    display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # CHUNK_SIZE = 1000\n",
    "# CHUNK_SIZE = 2000\n",
    "# # CHUNK_SIZE = 3500\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     # model_name=\"gpt-3.5-turbo\",\n",
    "#     chunk_size=CHUNK_SIZE,\n",
    "#     chunk_overlap=150\n",
    "# )\n",
    "\n",
    "# # Split the documents into chunks\n",
    "# doc1_chunks = text_splitter.split_text(doc1)\n",
    "# doc2_chunks = text_splitter.split_text(doc2)\n",
    "\n",
    "# # Display the number of chunks created\n",
    "# print(f\"Number of chunks in Document 1: {len(doc1_chunks)}\")\n",
    "# print(f\"Number of chunks in Document 2: {len(doc2_chunks)}\")\n",
    "\n",
    "# # Prepare the prompt for each chunk and collect responses\n",
    "# start_time = time.time()  # Start timing\n",
    "# responses = []\n",
    "# for i in range(max(len(doc1_chunks), len(doc2_chunks))):\n",
    "#     chunk1 = doc1_chunks[i] if i < len(doc1_chunks) else \"\"\n",
    "#     chunk2 = doc2_chunks[i] if i < len(doc2_chunks) else \"\"\n",
    "    \n",
    "#     prompt_chunk = prompt_template.format(doc1=chunk1, doc2=chunk2)\n",
    "#     num_tokens = count_openai_tokens(prompt=prompt_chunk)\n",
    "\n",
    "#     # Chat with SambaNova chat completion chain to structured output.\n",
    "#     structured_llm = llm.with_structured_output(Differences)\n",
    "\n",
    "#     # Invoke the model-to-structured-output chain using a prompt.\n",
    "#     completion = structured_llm.invoke(prompt_chunk)\n",
    "\n",
    "#     # Collect the response\n",
    "#     responses.append(completion.choices[0].message.content)\n",
    "\n",
    "# end_time = time.time()  # End timing\n",
    "# response_time_chunks = end_time - start_time  # Calculate response time\n",
    "# print(f\"Time to process chunks: {response_time_chunks}\")\n",
    "\n",
    "# failed with status code 200.', \"{'error': {'code': None, 'message': \n",
    "# 'Model DeepSeek-R1 does not support tool use.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks in Document 1: 2\n",
      "Number of chunks in Document 2: 2\n",
      "Number of tokens in the prompt: 6825\n",
      "Number of tokens in the prompt: 3740\n",
      "Time to process chunks: 10.227445840835571 seconds\n"
     ]
    }
   ],
   "source": [
    "# SambaNova native API uses OpenAI API\n",
    "from openai import OpenAI\n",
    "# https://github.com/sambanova/ai-starter-kit/blob/main/quickstart/README.md\n",
    "\n",
    "# Authenticate into SambaNova Cloud using API key\n",
    "api_key = os.environ.get(\"SAMBANOVA_API_KEY\")\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.sambanova.ai/v1/\",\n",
    "    api_key=api_key,  \n",
    ")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# CHUNK_SIZE = 1000\n",
    "# CHUNK_SIZE = 2000\n",
    "CHUNK_SIZE = 3500\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # model_name=\"gpt-3.5-turbo\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc1_chunks = text_splitter.split_text(doc1)\n",
    "doc2_chunks = text_splitter.split_text(doc2)\n",
    "\n",
    "# Display the number of chunks created\n",
    "print(f\"Number of chunks in Document 1: {len(doc1_chunks)}\")\n",
    "print(f\"Number of chunks in Document 2: {len(doc2_chunks)}\")\n",
    "\n",
    "# Prepare the prompt for each chunk and collect responses\n",
    "start_time = time.time()  # Start timing\n",
    "responses = []\n",
    "for i in range(max(len(doc1_chunks), len(doc2_chunks))):\n",
    "    chunk1 = doc1_chunks[i] if i < len(doc1_chunks) else \"\"\n",
    "    chunk2 = doc2_chunks[i] if i < len(doc2_chunks) else \"\"\n",
    "    \n",
    "    prompt_chunk = prompt_template.format(doc1=chunk1, doc2=chunk2)\n",
    "    num_tokens = count_openai_tokens(prompt=prompt_chunk)\n",
    "\n",
    "    # Call the OpenAI API to get the response for each chunk\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt_chunk,\n",
    "            }\n",
    "        ],\n",
    "        stream=streaming,\n",
    "    )\n",
    "    # Collect the response\n",
    "    responses.append(completion.choices[0].message.content)\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "response_time_chunks = end_time - start_time  # Calculate response time\n",
    "print(f\"Time to process chunks: {response_time_chunks} seconds\")\n",
    "\n",
    "# 37.26542592048645 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through list of responses.\n",
    "# i = 0\n",
    "# for diff in responses:\n",
    "#     pprint.pprint(f\"CHUNK {i}: {diff}\")\n",
    "#     print()\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive chunking approach needs 1 more prompt to combine chunks into single answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 4539\n"
     ]
    }
   ],
   "source": [
    "# Given chunk responses, combine into 1 single table\n",
    "\n",
    "# Assemble the prompt\n",
    "prompt_combine = f\"\"\" You are a highly skilled legal document analyst. \n",
    "Your task is to synthesize key contextual differences between 2 legal documents into a single, structured markdown table. \n",
    "\n",
    "Given an input string containing multiple separate chunk answers. \n",
    "Each chunk represents separate analysis for Doc1 and Doc2, related to different legal topics.\n",
    "Input string: {responses}\n",
    "\n",
    "**Objective:** \n",
    "Combine the information from these multiple input analyses into **one cohesive markdown table** with **only three rows**. \n",
    "This single output table will highlight the contextual differences between Doc1 and Doc2 across key features. \n",
    "\n",
    "**Instructions:** \n",
    "1. **Understand the differences in each chunk:**  \n",
    "\n",
    "2. **Focus on Key Topics:**  Your comparison should be centered around three critical topics common in the chunks: \n",
    "- Definition of Confidential Information\n",
    "- Permitted Use & Restrictions\n",
    "- Data Security\n",
    "\n",
    "3. **Analyze and Identify Contextual Differences:** \n",
    " Carefully analyze the clauses from Doc1 and Doc2 related to each of the topics above. \n",
    " Your goal is to identify and articulate the *contextual differences* in how each document addresses these areas.  \n",
    " Contextual differences refer to meaningful distinctions in the language, scope, or implications of the clauses in each document. \n",
    "\n",
    "Output JSON summarizing the contextual differences between Doc1 and Doc2 for each topic.\n",
    "JSON keys: \n",
    "topic, summary, doc1_context, doc2_context\n",
    "\n",
    "summary: \n",
    "Provide a concise summary of the key contextual difference between Doc1 and Doc2 \n",
    "for the specific feature. Focus on the *meaningful distinction* and its *practical implications*.\n",
    "\n",
    "doc1_context: \n",
    "Quote the *relevant text excerpt* from Doc1 that pertains to the feature. \n",
    "**Within this quoted text, use bold markdown formatting to highlight the specific words \n",
    "or phrases that are different or absent compared to the corresponding clause in Doc2.**\n",
    "\n",
    "doc2_context: \n",
    "Quote the *corresponding text excerpt* from Doc2 that addresses the same feature. \n",
    "**Within this quoted text, use bold markdown formatting to highlight the specific words \n",
    "or phrases that are different or absent compared to the clause in Doc1.**\n",
    "\n",
    "Use bold markdown to highlight the differing text within the \"doc1_context\" and \"doc2_context\" columns as described above.\n",
    "Make sure you highlight in bold for each row, only text differences, to make it easier for the user to see the differences.\n",
    "\"\"\"\n",
    "\n",
    "# Print number of tokens in the prompt\n",
    "num_tokens = count_openai_tokens(prompt_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time to combine chunks: 2.7475037574768066 seconds\n",
      "Total response time to combine chunks: 12.974949598312378 seconds\n",
      "[{\n",
      "        \"topic\": \"Definition of Confidential Information\",\n",
      "        \"summary\": \"Doc1 explicitly defines Confidential Information, while Doc2 does not provide a specific definition, which could lead to ambiguity in understanding what constitutes confidential information.\",\n",
      "        \"doc1_context\": \"“Confidential Information” shall mean all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure.\",\n",
      "        \"doc2_context\": \"No explicit definition of Confidential Information is provided in Doc2.\"\n",
      "    },\n",
      "    {\n",
      "        \"topic\": \"Permitted Use & Restrictions\",\n",
      "        \"summary\": \"Doc1 includes export control restrictions, while Doc2 focuses on proper usage and legal compliance without explicit permitted use guidelines.\",\n",
      "        \"doc1_context\": \"Customer agrees to comply with all export and import laws and regulations of the United States and other applicable jurisdictions. Without limiting the foregoing, (i) Customer represents and warrants that it is not listed on any U.S. government list of prohibited or restricted parties or located in (or a national of) a country that is subject to a U.S. government embargo or that has been designated by the U.S. government as a “terrorist supporting” country, (ii) Customer will not (and will not permit any third parties to) access or use any Service in violation of any U.S. export embargo, prohibition or restriction, and (iii) Customer will not submit to any Service any information that is controlled under the U.S. International Traffic in Arms Regulations.\",\n",
      "        \"doc2_context\": \"Unless prohibited by applicable law, if you are a business, you will defend and indemnify Google, and its affiliates, directors, officers, employees, and users, against all liabilities, damages, losses, costs, fees (including legal fees), and expenses relating to any allegation or third-party legal proceeding to the extent arising from: your misuse or your end user's misuse of the APIs; your violation or your end user's violation of the Terms; or any content or data routed into or used with the APIs by you, those acting on your behalf, or your end users.\"\n",
      "    },\n",
      "    {\n",
      "        \"topic\": \"Data Security\",\n",
      "        \"summary\": \"Doc1 indirectly addresses data security through hardware liability, while Doc2 does not provide explicit data security measures, leaving it to user responsibility.\",\n",
      "        \"doc1_context\": \"NOTHING IN THIS AGREEMENT EXCLUDES OR LIMITS EITHER PARTY’S LIABILITY FOR ... FAILURE TO RETURN, DAMAGE OR LOSS TO SAMBANOVA HARDWARE;\",\n",
      "        \"doc2_context\": \"No explicit mention of data security measures is provided in Doc2.\"\n",
      "    }]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| topic | summary | doc1_context | doc2_context |\n",
       "| --- | --- | --- | --- |\n",
       "| Definition of Confidential Information | Doc1 explicitly defines Confidential Information, while Doc2 does not provide a specific definition, which could lead to ambiguity in understanding what constitutes confidential information. | “Confidential Information” shall mean all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure. | No explicit definition of Confidential Information is provided in Doc2. |\n",
       "| Permitted Use & Restrictions | Doc1 includes export control restrictions, while Doc2 focuses on proper usage and legal compliance without explicit permitted use guidelines. | Customer agrees to comply with all export and import laws and regulations of the United States and other applicable jurisdictions. Without limiting the foregoing, (i) Customer represents and warrants that it is not listed on any U.S. government list of prohibited or restricted parties or located in (or a national of) a country that is subject to a U.S. government embargo or that has been designated by the U.S. government as a “terrorist supporting” country, (ii) Customer will not (and will not permit any third parties to) access or use any Service in violation of any U.S. export embargo, prohibition or restriction, and (iii) Customer will not submit to any Service any information that is controlled under the U.S. International Traffic in Arms Regulations. | Unless prohibited by applicable law, if you are a business, you will defend and indemnify Google, and its affiliates, directors, officers, employees, and users, against all liabilities, damages, losses, costs, fees (including legal fees), and expenses relating to any allegation or third-party legal proceeding to the extent arising from: your misuse or your end user's misuse of the APIs; your violation or your end user's violation of the Terms; or any content or data routed into or used with the APIs by you, those acting on your behalf, or your end users. |\n",
       "| Data Security | Doc1 indirectly addresses data security through hardware liability, while Doc2 does not provide explicit data security measures, leaving it to user responsibility. | NOTHING IN THIS AGREEMENT EXCLUDES OR LIMITS EITHER PARTY’S LIABILITY FOR ... FAILURE TO RETURN, DAMAGE OR LOSS TO SAMBANOVA HARDWARE; | No explicit mention of data security measures is provided in Doc2. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the OpenAI API to get the response for each chunk\n",
    "start_time = time.time()  # Start timing\n",
    "completion_combine = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt_combine,\n",
    "        }\n",
    "    ],\n",
    "    stream=streaming,\n",
    ")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "response_time = end_time - start_time  # Calculate response time\n",
    "print(f\"Response time to combine chunks: {response_time} seconds\")\n",
    "\n",
    "# Total response time = time for each chunk + combining chunks together\n",
    "total_response_time = response_time + response_time_chunks\n",
    "print(f\"Total response time to combine chunks: {total_response_time} seconds\")\n",
    "\n",
    "# Extract just the json structured output from a chat completion\n",
    "structured_output = extract_and_display_output(completion)\n",
    "\n",
    "# Display markdown table\n",
    "display_json_as_markdown(structured_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO (DeepSeek-R1) fine tuned Llama 3.3 70B\n",
    "128K Context Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Chat completion using the specified model.\n",
    "\n",
    "# # List all models\n",
    "# # https://cloud.sambanova.ai > Playground\n",
    "# model = \"DeepSeek-R1-Distill-Llama-70B\"\n",
    "\n",
    "# # Wrap LangChain model for SambaNova\n",
    "# llm = ChatSambaNovaCloud(\n",
    "#     model=model,\n",
    "#     max_tokens=1024,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.01,\n",
    "# )\n",
    "\n",
    "# start_time = time.time()  # Start timing\n",
    "\n",
    "# # Chat with SambaNova chat completion chain to structured output.\n",
    "# structured_llm = llm.with_structured_output(Differences)\n",
    "\n",
    "# # Invoke the model-to-structured-output chain using a prompt.\n",
    "# completion = structured_llm.invoke(prompt)\n",
    "\n",
    "# end_time = time.time()  # End timing\n",
    "# response_time = end_time - start_time  # Calculate response time\n",
    "# print(f\"Response time: {response_time} seconds for model: {model}\")\n",
    "\n",
    "# # Render the structured completion JSON as markdown table\n",
    "# display_differences_as_markdown(completion)\n",
    "\n",
    "# # failed with status code 200.', \"{'error': {'code': None, 'message': \n",
    "# # 'Model DeepSeek-R1-Distill-Llama-70B does not support tool use.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt: 10057\n",
      "[{\n",
      "        \"topic\": \"Definition of Confidential Information\",\n",
      "        \"summary\": \"Doc1 defines Confidential Information broadly, including any information reasonably considered confidential, and emphasizes equitable relief for breaches. Doc2 specifically excludes certain information and requires prior notice for legal disclosures.\",\n",
      "        \"doc1_context\": \"Confidential Information shall mean all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure. The Receiving Party must use the same degree of care as they would for their own confidential information, but not less than reasonable care. They can only use it for the scope of the agreement and must limit access to those who need it. If legally required to disclose, they must notify the Disclosing Party and cooperate to seek confidential treatment. Disclosing Confidential Information would cause substantial harm, so the Disclosing Party can seek equitable relief.\",\n",
      "        \"doc2_context\": \"Confidential Information includes developer credentials and Google's communications marked confidential. It explicitly excludes information independently developed, rightfully obtained from a third party without confidentiality, or that becomes public through no fault of the recipient. The Receiving Party must not disclose it to third parties without prior written consent, unless legally compelled, in which case they must provide prior notice unless a court prohibits it.\"\n",
      "    },\n",
      "    {\n",
      "        \"topic\": \"Permitted Use & Restrictions\",\n",
      "        \"summary\": \"Doc1 focuses on internal use and prohibits competing activities, while Doc2 has broader restrictions, especially on content handling and harmful activities.\",\n",
      "        \"doc1_context\": \"Allows use solely for Customer's benefit with a non-exclusive license. Prohibits reverse engineering, resale, benchmarking without consent, and competing product development. Requires User compliance.\",\n",
      "        \"doc2_context\": \"Permits API use to enhance applications but prohibits sublicensing, misrepresentation, illegal activities, reverse engineering, and harmful uses. Restricts content scraping and caching beyond limits.\"\n",
      "    },\n",
      "    {\n",
      "        \"topic\": \"Data Security\",\n",
      "        \"summary\": \"Doc1 shifts compliance responsibility to the Customer, while Doc2 requires active data protection and privacy policy adherence.\",\n",
      "        \"doc1_context\": \"Requires reasonable security measures and vulnerability reporting. SambaNova isn't responsible for legal compliance, placing the burden on the Customer.\",\n",
      "        \"doc2_context\": \"Mandates commercially reasonable efforts to protect user information, including personal data, with prompt breach reporting. Requires compliance with privacy laws and Google's User Data Policy, including a clear privacy policy.\"\n",
      "    }]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| topic | summary | doc1_context | doc2_context |\n",
       "| --- | --- | --- | --- |\n",
       "| Definition of Confidential Information | Doc1 defines Confidential Information broadly, including any information reasonably considered confidential, and emphasizes equitable relief for breaches. Doc2 specifically excludes certain information and requires prior notice for legal disclosures. | Confidential Information shall mean all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure. The Receiving Party must use the same degree of care as they would for their own confidential information, but not less than reasonable care. They can only use it for the scope of the agreement and must limit access to those who need it. If legally required to disclose, they must notify the Disclosing Party and cooperate to seek confidential treatment. Disclosing Confidential Information would cause substantial harm, so the Disclosing Party can seek equitable relief. | Confidential Information includes developer credentials and Google's communications marked confidential. It explicitly excludes information independently developed, rightfully obtained from a third party without confidentiality, or that becomes public through no fault of the recipient. The Receiving Party must not disclose it to third parties without prior written consent, unless legally compelled, in which case they must provide prior notice unless a court prohibits it. |\n",
       "| Permitted Use & Restrictions | Doc1 focuses on internal use and prohibits competing activities, while Doc2 has broader restrictions, especially on content handling and harmful activities. | Allows use solely for Customer's benefit with a non-exclusive license. Prohibits reverse engineering, resale, benchmarking without consent, and competing product development. Requires User compliance. | Permits API use to enhance applications but prohibits sublicensing, misrepresentation, illegal activities, reverse engineering, and harmful uses. Restricts content scraping and caching beyond limits. |\n",
       "| Data Security | Doc1 shifts compliance responsibility to the Customer, while Doc2 requires active data protection and privacy policy adherence. | Requires reasonable security measures and vulnerability reporting. SambaNova isn't responsible for legal compliance, placing the burden on the Customer. | Mandates commercially reasonable efforts to protect user information, including personal data, with prompt breach reporting. Requires compliance with privacy laws and Google's User Data Policy, including a clear privacy policy. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Chat completion using the specified model.\n",
    "\n",
    "# List all models\n",
    "# https://cloud.sambanova.ai > Playground\n",
    "model = \"DeepSeek-R1-Distill-Llama-70B\"\n",
    "\n",
    "# Print number of tokens in the prompt\n",
    "num_tokens = count_openai_tokens(prompt)\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt, # back to the original 10K length prompt\n",
    "        }\n",
    "    ],\n",
    "    stream=streaming,\n",
    ")\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "response_time = end_time - start_time  # Calculate response time\n",
    "print(f\"Time to process chunks: {response_time_chunks} seconds\")\n",
    "\n",
    "# Extract just the json structured output from a chat completion\n",
    "structured_output = extract_and_display_output(completion)\n",
    "\n",
    "# Display markdown table\n",
    "display_json_as_markdown(structured_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Together.ai\n",
    "\n",
    "- The [free](https://api.together.xyz/playground/chat/deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free) GRPO fine-tuned model only has 8K context length\n",
    "- The regular GRPO fine-tuned model has 128K context length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "        \"topic\": \"Definition of Confidential Information\",\n",
      "        \"summary\": \"Doc1 provides a broader definition of Confidential Information, focusing on the nature of the information and the circumstances of disclosure. Doc2 specifically includes developer credentials and communications, with detailed handling requirements.\",\n",
      "        \"doc1_context\": \"Confidential Information shall mean all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure. Each party (as “Receiving Party”) will use the same degree of care that it uses to protect the confidentiality of its own confidential information of like kind (but not less than reasonable care) to (i) not use any Confidential Information of the other party (“Disclosing Party”) for any purpose outside the scope of this Agreement, and (ii) except as otherwise authorized by the Disclosing Party in writing, limit access to Confidential Information of the Disclosing Party to those of its and its Affiliates’ employees and contractors who need that access for purposes consistent with this Agreement and who have signed confidentiality agreements with the Receiving Party containing protections not materially less protective of the Confidential Information than those herein. If Receiving Party is required by law or court order to disclose Confidential Information, then Receiving Party shall, to the extent legally permitted, provide Disclosing Party with advance written notification and cooperate in any effort to obtain confidential treatment of the Confidential Information. The Receiving Party acknowledges that disclosure of Confidential Information would cause substantial harm for which damages alone would not be a sufficient remedy, and therefore that upon any such violative disclosure by the Receiving Party, the Disclosing Party will be entitled to seek appropriate equitable relief in addition to any other remedies available at law.\",\n",
      "        \"doc2_context\": \"Confidential Information includes any materials, communications, and information that are marked confidential or that would normally be considered confidential under the circumstances. Developer credentials (such as passwords, keys, and client IDs) are intended to be used by you and identify your API Client. You will keep your credentials confidential and make reasonable efforts to prevent and discourage other API Clients from using your credentials. Developer credentials may not be embedded in open source projects. Our communications to you and our APIs may contain Google confidential information. If you receive any such information, then you will not disclose it to any third party without Google's prior written consent. Google confidential information does not include information that you independently developed, that was rightfully given to you by a third party without confidentiality obligation, or that becomes public through no fault of your own. You may disclose Google confidential information when compelled to do so by law if you provide us reasonable prior notice, unless a court orders that we not receive notice.\"\n",
      "    },\n",
      "    {\n",
      "        \"topic\": \"Permitted Use & Restrictions\",\n",
      "        \"summary\": \"Doc1 allows use for the Customer's benefit with general restrictions, while Doc2 has more specific restrictions, including prohibitions on certain activities and requirements for end-user compliance.\",\n",
      "        \"doc1_context\": \"The Service may include software, content, data, or other materials that are owned by persons or entities other than SambaNova and that are provided to Customer on licensee terms that are in addition to and/or different from those contained in this Agreement. Customer is bound by and shall comply with, and is responsible for ensuring that each User is bound by and complies with, all Third-Party Licenses. Any breach by Customer or any of its Users of any Third-Party License is also a breach of this Agreement.\",\n",
      "        \"doc2_context\": \"You will require your end users to comply with (and not knowingly enable them to violate) applicable law, regulation, and the Terms. You will comply with all applicable law, regulation, and third party rights (including without limitation laws regarding the import or export of data or software, privacy, and local laws). You will not use the APIs to encourage or promote illegal activity or violation of third party rights. You will not violate any other terms of service with Google (or its affiliates).\"\n",
      "    },\n",
      "    {\n",
      "        \"topic\": \"Data Security\",\n",
      "        \"summary\": \"Doc1 requires general security measures and places compliance burden on the Customer, whereas Doc2 provides detailed data protection requirements and privacy policies.\",\n",
      "        \"doc1_context\": \"You must implement and use reasonable and appropriate measures to help secure the Service by you and your Users. If you discover any vulnerabilities or breaches, you must promptly inform us and provide us details of such. As a generally available cloud compute service, we will provide the Service in accordance with our obligations under laws applicable to our Service generally, and without regard to your specific and particular use of the Service. If you use the Service to process personal data, or any other specified types of data requiring compliance with any applicable laws or regulations, you represent to us that you have established and maintained any such compliance, and have provided all applicable privacy notices and obtained necessary consents to process such data using the Service, and that you are processing such data using the Service in accordance with all applicable law. You understand and agree that we are not responsible for establishing, providing, obtaining, or maintaining any such compliance, notices and/or consents, and that we may not have done so.\",\n",
      "        \"doc2_context\": \"You will use commercially reasonable efforts to protect user information collected by your API Client, including personal data, from unauthorized access or use and will promptly report to your users any unauthorized access or use of such information to the extent required by applicable law. You will comply with (1) all applicable privacy laws and regulations including those applying to personal data and (2) the Google API Services User Data Policy, which governs your use of the APIs when you request access to Google user information. You will provide and adhere to a privacy policy for your API Client that clearly and accurately describes to users of your API Client what user information you collect and how you use and share such information (including for advertising) with Google and third parties.\"\n",
      "    }]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| topic | summary | doc1_context | doc2_context |\n",
       "| --- | --- | --- | --- |\n",
       "| Definition of Confidential Information | Doc1 provides a broader definition of Confidential Information, focusing on the nature of the information and the circumstances of disclosure. Doc2 specifically includes developer credentials and communications, with detailed handling requirements. | Confidential Information shall mean all information that is identified as confidential at the time of disclosure or should be reasonably known to be confidential or proprietary due to the nature of the information disclosed and the circumstances surrounding the disclosure. Each party (as “Receiving Party”) will use the same degree of care that it uses to protect the confidentiality of its own confidential information of like kind (but not less than reasonable care) to (i) not use any Confidential Information of the other party (“Disclosing Party”) for any purpose outside the scope of this Agreement, and (ii) except as otherwise authorized by the Disclosing Party in writing, limit access to Confidential Information of the Disclosing Party to those of its and its Affiliates’ employees and contractors who need that access for purposes consistent with this Agreement and who have signed confidentiality agreements with the Receiving Party containing protections not materially less protective of the Confidential Information than those herein. If Receiving Party is required by law or court order to disclose Confidential Information, then Receiving Party shall, to the extent legally permitted, provide Disclosing Party with advance written notification and cooperate in any effort to obtain confidential treatment of the Confidential Information. The Receiving Party acknowledges that disclosure of Confidential Information would cause substantial harm for which damages alone would not be a sufficient remedy, and therefore that upon any such violative disclosure by the Receiving Party, the Disclosing Party will be entitled to seek appropriate equitable relief in addition to any other remedies available at law. | Confidential Information includes any materials, communications, and information that are marked confidential or that would normally be considered confidential under the circumstances. Developer credentials (such as passwords, keys, and client IDs) are intended to be used by you and identify your API Client. You will keep your credentials confidential and make reasonable efforts to prevent and discourage other API Clients from using your credentials. Developer credentials may not be embedded in open source projects. Our communications to you and our APIs may contain Google confidential information. If you receive any such information, then you will not disclose it to any third party without Google's prior written consent. Google confidential information does not include information that you independently developed, that was rightfully given to you by a third party without confidentiality obligation, or that becomes public through no fault of your own. You may disclose Google confidential information when compelled to do so by law if you provide us reasonable prior notice, unless a court orders that we not receive notice. |\n",
       "| Permitted Use & Restrictions | Doc1 allows use for the Customer's benefit with general restrictions, while Doc2 has more specific restrictions, including prohibitions on certain activities and requirements for end-user compliance. | The Service may include software, content, data, or other materials that are owned by persons or entities other than SambaNova and that are provided to Customer on licensee terms that are in addition to and/or different from those contained in this Agreement. Customer is bound by and shall comply with, and is responsible for ensuring that each User is bound by and complies with, all Third-Party Licenses. Any breach by Customer or any of its Users of any Third-Party License is also a breach of this Agreement. | You will require your end users to comply with (and not knowingly enable them to violate) applicable law, regulation, and the Terms. You will comply with all applicable law, regulation, and third party rights (including without limitation laws regarding the import or export of data or software, privacy, and local laws). You will not use the APIs to encourage or promote illegal activity or violation of third party rights. You will not violate any other terms of service with Google (or its affiliates). |\n",
       "| Data Security | Doc1 requires general security measures and places compliance burden on the Customer, whereas Doc2 provides detailed data protection requirements and privacy policies. | You must implement and use reasonable and appropriate measures to help secure the Service by you and your Users. If you discover any vulnerabilities or breaches, you must promptly inform us and provide us details of such. As a generally available cloud compute service, we will provide the Service in accordance with our obligations under laws applicable to our Service generally, and without regard to your specific and particular use of the Service. If you use the Service to process personal data, or any other specified types of data requiring compliance with any applicable laws or regulations, you represent to us that you have established and maintained any such compliance, and have provided all applicable privacy notices and obtained necessary consents to process such data using the Service, and that you are processing such data using the Service in accordance with all applicable law. You understand and agree that we are not responsible for establishing, providing, obtaining, or maintaining any such compliance, notices and/or consents, and that we may not have done so. | You will use commercially reasonable efforts to protect user information collected by your API Client, including personal data, from unauthorized access or use and will promptly report to your users any unauthorized access or use of such information to the extent required by applicable law. You will comply with (1) all applicable privacy laws and regulations including those applying to personal data and (2) the Google API Services User Data Policy, which governs your use of the APIs when you request access to Google user information. You will provide and adhere to a privacy policy for your API Client that clearly and accurately describes to users of your API Client what user information you collect and how you use and share such information (including for advertising) with Google and third parties. |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "client = Together()\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "# Using paid GRPO DeepSeek R1 endpoint since it supports longer context length\n",
    "response = client.chat.completions.create(\n",
    "    # model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "    max_tokens=10200,  # default=None\n",
    "    temperature=0.7,\n",
    "    top_p=0.7,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1,\n",
    "    stop=[\"<｜end▁of▁sentence｜>\"],\n",
    "    stream=False\n",
    ")\n",
    "end_time = time.time()  # End timing\n",
    "response_time = end_time - start_time  # Calculate response time\n",
    "print(f\"Time to process chunks: {response_time} seconds\")\n",
    "\n",
    "# Extract just the json structured output from a chat completion\n",
    "structured_output = extract_and_display_output(response)\n",
    "\n",
    "# Display markdown table\n",
    "display_json_as_markdown(structured_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
